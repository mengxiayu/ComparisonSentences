12/19/2022 23:14:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
12/19/2022 23:14:19 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=256,
generation_num_beams=None,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/multinews_abstractive/compbart12114_01/runs/Dec19_23-14-18_ta-a6k-002.crc.nd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/multinews_abstractive/compbart12114_01,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=16,
per_device_train_batch_size=16,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/multinews_abstractive/compbart12114_01,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
12/19/2022 23:14:19 - WARNING - datasets.builder - Using custom data configuration default-00cd43ebba0a2f1d
12/19/2022 23:14:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.
12/19/2022 23:14:19 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-00cd43ebba0a2f1d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
12/19/2022 23:14:19 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-00cd43ebba0a2f1d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
12/19/2022 23:14:19 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-00cd43ebba0a2f1d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
12/19/2022 23:14:21 - WARNING - datasets.builder - Using custom data configuration default-b04715d0fcf24e6d
12/19/2022 23:14:21 - INFO - datasets.builder - Overwrite dataset info from restored data version.
12/19/2022 23:14:21 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-b04715d0fcf24e6d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
12/19/2022 23:14:21 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-b04715d0fcf24e6d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
12/19/2022 23:14:21 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-b04715d0fcf24e6d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
12/19/2022 23:14:22 - WARNING - datasets.builder - Using custom data configuration default-4e05394053c39697
12/19/2022 23:14:22 - INFO - datasets.builder - Overwrite dataset info from restored data version.
12/19/2022 23:14:22 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-4e05394053c39697/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
12/19/2022 23:14:22 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-4e05394053c39697/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
12/19/2022 23:14:22 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-4e05394053c39697/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/torch/cuda/__init__.py:104: UserWarning: 
NVIDIA RTX A6000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75.
If you want to use the NVIDIA RTX A6000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
[INFO|configuration_utils.py:646] 2022-12-19 23:14:22,634 >> loading configuration file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/config.json
[INFO|configuration_utils.py:684] 2022-12-19 23:14:22,649 >> Model config BartConfig {
  "_name_or_path": "/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1703] 2022-12-19 23:14:22,671 >> Didn't find file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1784] 2022-12-19 23:14:22,672 >> loading file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/vocab.json
[INFO|tokenization_utils_base.py:1784] 2022-12-19 23:14:22,672 >> loading file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/merges.txt
[INFO|tokenization_utils_base.py:1784] 2022-12-19 23:14:22,672 >> loading file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/tokenizer.json
[INFO|tokenization_utils_base.py:1784] 2022-12-19 23:14:22,672 >> loading file None
[INFO|tokenization_utils_base.py:1784] 2022-12-19 23:14:22,672 >> loading file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/special_tokens_map.json
[INFO|tokenization_utils_base.py:1784] 2022-12-19 23:14:22,672 >> loading file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/tokenizer_config.json
[INFO|modeling_utils.py:1429] 2022-12-19 23:14:23,093 >> loading weights file /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160/pytorch_model.bin
[INFO|modeling_utils.py:1702] 2022-12-19 23:14:28,478 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.

[INFO|modeling_utils.py:1711] 2022-12-19 23:14:28,478 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pretrain_news_gen/combined_1213/checkpoint-5160.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
Running tokenizer on train dataset #0:   0%|          | 0/5 [00:00<?, ?ba/s]
Running tokenizer on train dataset #1:   0%|          | 0/5 [00:00<?, ?ba/s][A

Running tokenizer on train dataset #2:   0%|          | 0/5 [00:00<?, ?ba/s][A[A


Running tokenizer on train dataset #3:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A



Running tokenizer on train dataset #4:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on train dataset #5:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on train dataset #6:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on train dataset #7:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on train dataset #8:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on train dataset #9:   0%|          | 0/5 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A





Running tokenizer on train dataset #6:  20%|â–ˆâ–ˆ        | 1/5 [00:08<00:33,  8.28s/ba][A[A[A[A[A[A




Running tokenizer on train dataset #5:  20%|â–ˆâ–ˆ        | 1/5 [00:08<00:34,  8.75s/ba][A[A[A[A[A






Running tokenizer on train dataset #7:  20%|â–ˆâ–ˆ        | 1/5 [00:08<00:35,  8.87s/ba][A[A[A[A[A[A[A







Running tokenizer on train dataset #8:  20%|â–ˆâ–ˆ        | 1/5 [00:08<00:35,  8.87s/ba][A[A[A[A[A[A[A[A


Running tokenizer on train dataset #3:  20%|â–ˆâ–ˆ        | 1/5 [00:09<00:36,  9.12s/ba][A[A[A



Running tokenizer on train dataset #4:  20%|â–ˆâ–ˆ        | 1/5 [00:09<00:36,  9.13s/ba][A[A[A[ARunning tokenizer on train dataset #0:  20%|â–ˆâ–ˆ        | 1/5 [00:09<00:37,  9.34s/ba]

Running tokenizer on train dataset #2:  20%|â–ˆâ–ˆ        | 1/5 [00:09<00:37,  9.27s/ba][A[A
Running tokenizer on train dataset #1:  20%|â–ˆâ–ˆ        | 1/5 [00:09<00:37,  9.35s/ba][A








Running tokenizer on train dataset #9:  20%|â–ˆâ–ˆ        | 1/5 [00:09<00:36,  9.23s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on train dataset #6:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:16<00:24,  8.23s/ba][A[A[A[A[A[A




Running tokenizer on train dataset #5:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:17<00:26,  8.75s/ba][A[A[A[A[A







Running tokenizer on train dataset #8:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:17<00:26,  8.73s/ba][A[A[A[A[A[A[A[A



Running tokenizer on train dataset #4:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:17<00:26,  8.78s/ba][A[A[A[A






Running tokenizer on train dataset #7:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:17<00:26,  8.80s/ba][A[A[A[A[A[A[A


Running tokenizer on train dataset #3:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:17<00:26,  8.87s/ba][A[A[A
Running tokenizer on train dataset #1:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:27,  9.08s/ba][A








Running tokenizer on train dataset #9:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:27,  9.01s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on train dataset #2:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:27,  9.18s/ba][A[ARunning tokenizer on train dataset #0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:18<00:27,  9.25s/ba]





Running tokenizer on train dataset #6:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:25<00:16,  8.38s/ba][A[A[A[A[A[A




Running tokenizer on train dataset #5:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:25<00:17,  8.53s/ba][A[A[A[A[A






Running tokenizer on train dataset #7:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:26<00:17,  8.65s/ba][A[A[A[A[A[A[A



Running tokenizer on train dataset #4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:26<00:17,  8.69s/ba][A[A[A[A


Running tokenizer on train dataset #3:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:26<00:17,  8.82s/ba][A[A[A







Running tokenizer on train dataset #8:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:26<00:18,  9.07s/ba][A[A[A[A[A[A[A[A

Running tokenizer on train dataset #2:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:27<00:18,  9.09s/ba][A[A








Running tokenizer on train dataset #9:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:27<00:18,  9.02s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on train dataset #0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:27<00:18,  9.19s/ba]
Running tokenizer on train dataset #1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:27<00:18,  9.30s/ba][A





Running tokenizer on train dataset #6:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:33<00:08,  8.52s/ba][A[A[A[A[A[A




Running tokenizer on train dataset #5:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:34<00:08,  8.43s/ba][A[A[A[A[A


Running tokenizer on train dataset #3:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:34<00:08,  8.63s/ba][A[A[A



Running tokenizer on train dataset #4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:34<00:08,  8.71s/ba][A[A[A[A






Running tokenizer on train dataset #7:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:35<00:08,  8.93s/ba][A[A[A[A[A[A[ARunning tokenizer on train dataset #0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:36<00:08,  8.92s/ba]







Running tokenizer on train dataset #8:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:36<00:09,  9.06s/ba][A[A[A[A[A[A[A[A

Running tokenizer on train dataset #2:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:36<00:09,  9.01s/ba][A[A








Running tokenizer on train dataset #9:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:36<00:09,  9.04s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on train dataset #1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:36<00:09,  9.09s/ba][A





Running tokenizer on train dataset #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:38<00:00,  7.13s/ba][A[A[A[A[A[ARunning tokenizer on train dataset #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:38<00:00,  7.69s/ba]


Running tokenizer on train dataset #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:39<00:00,  7.03s/ba][A[A[ARunning tokenizer on train dataset #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:39<00:00,  7.82s/ba]




Running tokenizer on train dataset #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:39<00:00,  7.48s/ba][A[A[A[A[ARunning tokenizer on train dataset #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:39<00:00,  7.97s/ba]






Running tokenizer on train dataset #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:39<00:00,  7.27s/ba][A[A[A[A[A[A[ARunning tokenizer on train dataset #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:39<00:00,  7.95s/ba]







Running tokenizer on train dataset #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:40<00:00,  7.40s/ba][A[A[A[A[A[A[A[ARunning tokenizer on train dataset #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:40<00:00,  8.09s/ba]








Running tokenizer on train dataset #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:40<00:00,  7.32s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on train dataset #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:40<00:00,  8.09s/ba]

Running tokenizer on train dataset #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:40<00:00,  7.42s/ba][A[ARunning tokenizer on train dataset #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:40<00:00,  8.17s/ba]
Running tokenizer on train dataset #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:41<00:00,  7.48s/ba][ARunning tokenizer on train dataset #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:41<00:00,  8.24s/ba]Running tokenizer on train dataset #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:41<00:00,  7.67s/ba]Running tokenizer on train dataset #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:41<00:00,  8.32s/ba]



Running tokenizer on train dataset #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:42<00:00,  8.11s/ba][A[A[A[ARunning tokenizer on train dataset #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:42<00:00,  8.41s/ba]







12/19/2022 23:15:11 - INFO - __main__ - Sample 41905 of the training set: {'input_ids': [0, 1437, 1437, 1437, 166, 17, 27, 548, 70, 57, 174, 7, 3529, 84, 8942, 6, 8, 190, 114, 52, 218, 17, 27, 90, 101, 24, 6, 52, 216, 51, 17, 27, 241, 205, 13, 201, 4, 125, 10, 92, 892, 924, 95, 141, 205, 13, 84, 23610, 51, 189, 28, 4, 1437, 1437, 7732, 50, 55, 14566, 9, 6231, 8, 8942, 10, 183, 64, 795, 110, 810, 9, 8180, 30, 41, 19720, 3330, 4234, 309, 7, 10, 92, 892, 1027, 11, 5, 3642, 9, 42443, 17129, 359, 2573, 1309, 4, 20, 55, 12849, 8, 8942, 5, 3597, 14964, 6, 5, 540, 533, 51, 58, 7, 1597, 23, 143, 1046, 6, 8, 5, 11775, 1796, 1130, 19, 4850, 4, 20, 121, 4, 104, 4, 641, 9, 8004, 10827, 4558, 227, 65, 7, 80, 12988, 9, 6231, 1230, 8, 65, 7, 130, 12988, 9, 8942, 1230, 6, 6122, 15, 1046, 8, 3959, 4, 2667, 16760, 3905, 6, 44, 48, 597, 26491, 8, 32065, 93, 55, 3510, 4, 17, 46, 1221, 18192, 4441, 80, 14566, 9, 6231, 8, 292, 9, 8942, 6, 8, 11, 5, 121, 4, 530, 482, 5, 16760, 16, 35, 44, 48, 245, 10, 183, 4, 17, 46, 1437, 1437, 1437, 1437, 520, 1118, 19, 16997, 540, 87, 65, 4745, 9, 6231, 8, 8942, 10, 183, 6, 5, 810, 9, 744, 30, 143, 1303, 21, 2906, 30, 501, 207, 30, 4441, 65, 7, 130, 14566, 131, 1132, 207, 13, 130, 7, 292, 14566, 131, 2491, 207, 13, 292, 7, 707, 14566, 131, 8, 3330, 207, 13, 707, 50, 55, 4, 30437, 707, 50, 55, 14566, 67, 4010, 2906, 5, 810, 9, 8180, 31, 1668, 30, 564, 4234, 8, 1144, 2199, 30, 1105, 2153, 1437, 1437, 44, 48, 133, 699, 1579, 259, 16, 14, 5, 55, 6231, 8, 8942, 47, 3529, 6, 5, 540, 533, 47, 32, 7, 1597, 23, 143, 1046, 6, 17, 46, 483, 892, 2730, 17311, 179, 462, 3019, 17311, 3209, 4636, 6, 9, 589, 1821, 928, 17, 27, 29, 641, 9, 42443, 17129, 8, 1909, 1309, 6, 26, 11, 10, 445, 4, 44, 48, 30660, 6460, 6058, 33, 10, 2514, 1683, 87, 6231, 6, 53, 6231, 202, 817, 10, 588, 2249, 4, 318, 47, 17, 27, 241, 1372, 7, 17687, 15, 28488, 50, 97, 8942, 6, 172, 14, 16, 10, 372, 2031, 6, 53, 114, 47, 13185, 402, 24043, 5906, 6, 10, 23611, 50, 143, 6231, 40, 67, 109, 47, 205, 4, 17, 46, 1437, 1437, 20, 892, 16, 5, 78, 7, 6754, 4441, 12849, 8, 8942, 19, 70, 12, 27037, 6, 1668, 8, 1144, 12, 417, 1496, 3175, 3257, 11, 10, 9852, 4915, 1956, 6, 8, 7, 30127, 5, 1795, 30, 14566, 4, 11161, 1415, 23, 775, 31, 5, 1309, 11624, 13, 1156, 227, 5155, 8, 1014, 6, 61, 4271, 5, 4441, 10095, 9, 3620, 6, 29190, 82, 4, 1437, 1437, 4934, 14566, 9, 6231, 8, 8942, 10, 183, 126, 10, 2950, 23157, 13, 167, 2273, 59, 49, 308, 8, 49, 408, 18, 474, 126, 189, 45, 6, 71, 70, 6, 28, 615, 6, 309, 7, 10, 92, 266, 30, 4211, 6, 54, 3608, 52, 197, 1386, 28, 9998, 13, 707, 10, 183, 6, 8, 2260, 8942, 23, 14, 4, 726, 4526, 7790, 13, 103, 54, 802, 51, 58, 608, 5, 235, 631, 6, 12899, 7456, 8, 9214, 6231, 189, 45, 28, 7163, 23, 70, 4, 1437, 1437, 20, 665, 12320, 126, 8045, 7, 1693, 10, 11491, 260, 31, 167, 416, 33708, 196, 81, 1652, 9, 1985, 34625, 8, 25744, 4740, 26425, 126, 28125, 31, 10, 892, 2584, 66, 30, 2320, 23, 589, 1821, 928, 6, 54, 24305, 5, 4441, 10095, 9, 3620, 6, 151, 82, 6, 1487, 149, 799, 107, 9, 5, 1309, 11624, 13, 1156, 6, 8, 9184, 106, 19, 4685, 9, 744, 4, 1437, 1437, 20, 699, 2609, 21, 14, 4441, 55, 2310, 6231, 8, 8942, 6, 217, 26924, 6, 21, 3307, 7, 1207, 10, 1181, 301, 3489, 8, 11, 1989, 6, 7, 10, 795, 778, 9, 744, 31, 1144, 2199, 6, 8579, 8, 1668, 4, 1437, 1437, 30437, 23, 513, 707, 14566, 9, 2310, 6231, 8, 8942, 10, 183, 21, 3307, 7, 10, 3330, 207, 795, 810, 9, 744, 31, 70, 4685, 4, 85, 21, 67, 3059, 19, 10, 564, 207, 795, 810, 9, 1668, 8, 1105, 207, 795, 810, 9, 1144, 2199, 50, 8579, 4, 32912, 6058, 2551, 7, 28, 3625, 55, 2591, 136, 2199, 87, 4441, 6231, 6, 51, 224, 4, 1437, 1437, 345, 21, 10, 2755, 2609, 126, 82, 54, 14964, 24623, 50, 9214, 6231, 888, 56, 10, 723, 810, 9, 1144, 2199, 6, 8579, 8, 1668, 4, 1437, 1437, 20, 7601, 6, 925, 17311, 179, 462, 3019, 17311, 3209, 4636, 8, 4025, 31, 5, 1494, 9, 29027, 17129, 8, 285, 474, 23, 121, 7454, 6, 26, 51, 58, 17118, 141, 7, 18107, 5, 4139, 15, 24623, 50, 9214, 6231, 479, 85, 115, 28, 14, 82, 4441, 24623, 6231, 189, 45, 697, 11, 911, 147, 89, 16, 2310, 6231, 11, 5, 6464, 6, 61, 115, 6364, 10, 22555, 5626, 4, 1437, 1437, 28013, 6, 51, 115, 28, 82, 54, 32, 416, 11, 4812, 12, 14388, 50, 51, 115, 483, 37, 20069, 28182, 4, 345, 16, 67, 277, 3302, 35, 9214, 8, 12899, 7456, 6231, 58, 38015, 561, 11, 5, 1142, 6, 53, 150, 9214, 6231, 16, 1687, 7, 28, 17145, 338, 18579, 5, 276, 25, 2310, 6, 12899, 7456, 6231, 16, 10696, 11, 21580, 8200, 1823, 4696, 4, 901, 173, 782, 7, 28, 626, 7, 192, 549, 4045, 4490, 6, 12899, 7456, 6231, 16, 11, 754, 5, 696, 6, 5, 2634, 224, 4, 1437, 1437, 17311, 3209, 4636, 8, 4025, 362, 88, 1316, 5, 20182, 12, 12063, 3618, 6, 7893, 10095, 8, 97, 6339, 2433, 14, 3327, 82, 18, 474, 4, 653, 51, 33, 303, 6, 51, 224, 6, 16, 10, 670, 5259, 227, 239, 1389, 9, 6231, 8, 16090, 4850, 8, 795, 16374, 744, 1162, 126, 45, 10, 41214, 1291, 4, 1437, 1437, 125, 5, 2707, 9, 5, 892, 6, 1027, 11, 5, 3642, 9, 42443, 17129, 8, 2573, 1309, 6, 16, 11, 5, 380, 1530, 8, 5, 754, 14, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2383, 1648, 114, 47, 214, 10, 2602, 2267, 9, 292, 37116, 9, 12849, 8, 8942, 1230, 6, 47, 214, 45, 562, 615, 6, 41, 4935, 1089, 892, 3649, 4, 178, 16452, 699, 9, 5, 24623, 2682, 4, 589, 1821, 928, 2634, 7123, 5, 22669, 9, 3620, 6, 151, 82, 131, 51, 303, 14, 167, 54, 14964, 55, 2310, 12849, 8, 8942, 21131, 7, 697, 1181, 6, 8, 5, 8375, 11543, 2551, 7, 28, 707, 12, 7269, 37116, 358, 183, 4, 2246, 54, 478, 14, 672, 2906, 49, 810, 9, 744, 22, 7761, 70, 4685, 113, 30, 3330, 207, 25, 1118, 7, 167, 54, 14964, 540, 87, 65, 4745, 1230, 6, 5, 8137, 690, 4, 901, 4010, 6, 167, 380, 15, 12849, 8, 32065, 794, 10, 1105, 207, 795, 810, 9, 1144, 2199, 8, 10, 564, 207, 795, 810, 9, 1668, 4, 289, 9451, 292, 16, 6, 9, 768, 6, 202, 10142, 35, 21213, 130, 7, 292, 14566, 6, 8, 1374, 810, 9305, 1132, 4234, 150, 292, 7, 707, 839, 10, 2491, 207, 795, 810, 6, 3421, 690, 4, 96, 5, 382, 6, 5, 2526, 16760, 77, 24, 606, 7, 12849, 8, 8942, 16, 22, 4321, 3510, 60, 53, 5, 275, 2949, 16, 1221, 18, 6, 161, 10, 9338, 131, 11, 14, 247, 6, 5, 2949, 16, 22, 2977, 13, 132, 2744, 245, 60, 50, 80, 37116, 9, 12849, 8, 292, 9, 8942, 4, 22, 7605, 84, 892, 24, 1326, 101, 8942, 32, 357, 87, 6231, 60, 79, 161, 4, 1456, 2449, 35, 230, 12788, 2]}.
Running tokenizer on validation dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]
Running tokenizer on validation dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s][A

Running tokenizer on validation dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s][A[A


Running tokenizer on validation dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A



Running tokenizer on validation dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on validation dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on validation dataset #6:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on validation dataset #7:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on validation dataset #8:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on validation dataset #9:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A






Running tokenizer on validation dataset #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.24s/ba][A[A[A[A[A[A[ARunning tokenizer on validation dataset #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.24s/ba]





Running tokenizer on validation dataset #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/ba][A[A[A[A[A[ARunning tokenizer on validation dataset #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/ba]




Running tokenizer on validation dataset #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/ba][A[A[A[A[ARunning tokenizer on validation dataset #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/ba]

Running tokenizer on validation dataset #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.58s/ba][A[ARunning tokenizer on validation dataset #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.58s/ba]








Running tokenizer on validation dataset #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on validation dataset #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/ba]
Running tokenizer on validation dataset #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.78s/ba][ARunning tokenizer on validation dataset #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.78s/ba]


Running tokenizer on validation dataset #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/ba][A[A[ARunning tokenizer on validation dataset #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/ba]



Running tokenizer on validation dataset #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.21s/ba][A[A[A[ARunning tokenizer on validation dataset #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.21s/ba]Running tokenizer on validation dataset #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.36s/ba]Running tokenizer on validation dataset #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.36s/ba]







Running tokenizer on validation dataset #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.20s/ba][A[A[A[A[A[A[A[ARunning tokenizer on validation dataset #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.20s/ba]








Running tokenizer on prediction dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]
Running tokenizer on prediction dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s][A

Running tokenizer on prediction dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s][A[A


Running tokenizer on prediction dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A



Running tokenizer on prediction dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on prediction dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on prediction dataset #6:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on prediction dataset #7:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on prediction dataset #8:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on prediction dataset #9:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A





Running tokenizer on prediction dataset #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/ba][A[A[A[A[A[ARunning tokenizer on prediction dataset #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/ba]
Running tokenizer on prediction dataset #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.78s/ba][ARunning tokenizer on prediction dataset #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.78s/ba]



Running tokenizer on prediction dataset #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.81s/ba][A[A[A[ARunning tokenizer on prediction dataset #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.82s/ba]Running tokenizer on prediction dataset #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.97s/ba]Running tokenizer on prediction dataset #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.97s/ba]


Running tokenizer on prediction dataset #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.99s/ba][A[A[ARunning tokenizer on prediction dataset #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.99s/ba]








Running tokenizer on prediction dataset #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.85s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on prediction dataset #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.85s/ba]

Running tokenizer on prediction dataset #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.16s/ba][A[ARunning tokenizer on prediction dataset #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.16s/ba]




Running tokenizer on prediction dataset #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.32s/ba][A[A[A[A[ARunning tokenizer on prediction dataset #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.32s/ba]







Running tokenizer on prediction dataset #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.22s/ba][A[A[A[A[A[A[A[ARunning tokenizer on prediction dataset #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.22s/ba]






Running tokenizer on prediction dataset #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.04s/ba][A[A[A[A[A[A[ARunning tokenizer on prediction dataset #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.04s/ba]








/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1279] 2022-12-19 23:15:47,390 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-12-19 23:15:47,390 >>   Num examples = 44972
[INFO|trainer.py:1281] 2022-12-19 23:15:47,390 >>   Num Epochs = 5
[INFO|trainer.py:1282] 2022-12-19 23:15:47,390 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1283] 2022-12-19 23:15:47,390 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-12-19 23:15:47,390 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1285] 2022-12-19 23:15:47,390 >>   Total optimization steps = 7025
  0%|          | 0/7025 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run_seq2seq.py", line 685, in <module>
    main()
  File "run_seq2seq.py", line 621, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/trainer.py", line 1400, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/trainer.py", line 1984, in training_step
    loss = self.compute_loss(model, inputs)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/trainer.py", line 2016, in compute_loss
    outputs = model(**inputs)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py", line 1344, in forward
    return_dict=return_dict,
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py", line 1205, in forward
    return_dict=return_dict,
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py", line 778, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
RuntimeError: CUDA error: no kernel image is available for execution on the device
  0%|          | 0/7025 [00:00<?, ?it/s]