10/19/2022 18:51:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
10/19/2022 18:51:32 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
generation_max_length=64,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/runs/Oct19_18-51-32_qa-2080ti-006.crc.nd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=2.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
10/19/2022 18:51:32 - WARNING - datasets.builder - Using custom data configuration default-6cb3097592fe410a
10/19/2022 18:51:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.
10/19/2022 18:51:32 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-6cb3097592fe410a/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
10/19/2022 18:51:32 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-6cb3097592fe410a/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
10/19/2022 18:51:32 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-6cb3097592fe410a/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
10/19/2022 18:51:32 - WARNING - datasets.builder - Using custom data configuration default-3216209026870aea
10/19/2022 18:51:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.
10/19/2022 18:51:32 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-3216209026870aea/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
10/19/2022 18:51:32 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-3216209026870aea/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
10/19/2022 18:51:32 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-3216209026870aea/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
[INFO|configuration_utils.py:648] 2022-10-19 18:51:32,953 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910
[INFO|configuration_utils.py:684] 2022-10-19 18:51:32,954 >> Model config BartConfig {
  "_name_or_path": "facebook/bart-base",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:344] 2022-10-19 18:51:33,082 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-10-19 18:51:33,219 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910
[INFO|configuration_utils.py:684] 2022-10-19 18:51:33,220 >> Model config BartConfig {
  "_name_or_path": "facebook/bart-base",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1786] 2022-10-19 18:51:34,137 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1786] 2022-10-19 18:51:34,138 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1786] 2022-10-19 18:51:34,138 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1786] 2022-10-19 18:51:34,138 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-10-19 18:51:34,138 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-10-19 18:51:34,138 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-10-19 18:51:34,272 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910
[INFO|configuration_utils.py:684] 2022-10-19 18:51:34,273 >> Model config BartConfig {
  "_name_or_path": "facebook/bart-base",
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "forced_bos_token_id": 0,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:1431] 2022-10-19 18:51:34,516 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd
[INFO|modeling_utils.py:1702] 2022-10-19 18:51:36,684 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.

[INFO|modeling_utils.py:1711] 2022-10-19 18:51:36,684 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.
Running tokenizer on train dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]
Running tokenizer on train dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s][A

Running tokenizer on train dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s][A[A


Running tokenizer on train dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A



Running tokenizer on train dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on train dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on train dataset #6:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on train dataset #7:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on train dataset #8:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on train dataset #9:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on train dataset #0: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ba]Running tokenizer on train dataset #0: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ba]

Running tokenizer on train dataset #2: 100%|██████████| 1/1 [00:01<00:00,  1.70s/ba][A[ARunning tokenizer on train dataset #2: 100%|██████████| 1/1 [00:01<00:00,  1.70s/ba]
Running tokenizer on train dataset #1: 100%|██████████| 1/1 [00:01<00:00,  1.81s/ba][ARunning tokenizer on train dataset #1: 100%|██████████| 1/1 [00:01<00:00,  1.81s/ba]






Running tokenizer on train dataset #7: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ba][A[A[A[A[A[A[ARunning tokenizer on train dataset #7: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ba]



Running tokenizer on train dataset #4: 100%|██████████| 1/1 [00:01<00:00,  1.88s/ba][A[A[A[ARunning tokenizer on train dataset #4: 100%|██████████| 1/1 [00:01<00:00,  1.88s/ba]




Running tokenizer on train dataset #5: 100%|██████████| 1/1 [00:01<00:00,  1.84s/ba][A[A[A[A[ARunning tokenizer on train dataset #5: 100%|██████████| 1/1 [00:01<00:00,  1.84s/ba]


Running tokenizer on train dataset #3: 100%|██████████| 1/1 [00:01<00:00,  1.98s/ba][A[A[ARunning tokenizer on train dataset #3: 100%|██████████| 1/1 [00:01<00:00,  1.98s/ba]








Running tokenizer on train dataset #9: 100%|██████████| 1/1 [00:01<00:00,  1.87s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on train dataset #9: 100%|██████████| 1/1 [00:01<00:00,  1.87s/ba]





Running tokenizer on train dataset #6: 100%|██████████| 1/1 [00:02<00:00,  2.27s/ba][A[A[A[A[A[ARunning tokenizer on train dataset #6: 100%|██████████| 1/1 [00:02<00:00,  2.27s/ba]







Running tokenizer on train dataset #8: 100%|██████████| 1/1 [00:02<00:00,  2.27s/ba][A[A[A[A[A[A[A[ARunning tokenizer on train dataset #8: 100%|██████████| 1/1 [00:02<00:00,  2.27s/ba]






Running tokenizer on validation dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]
Running tokenizer on validation dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s][A

Running tokenizer on validation dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s][A[A


Running tokenizer on validation dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A



Running tokenizer on validation dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A




Running tokenizer on validation dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A





Running tokenizer on validation dataset #6:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A






Running tokenizer on validation dataset #7:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A







Running tokenizer on validation dataset #8:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A








Running tokenizer on validation dataset #9:   0%|          | 0/1 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A
Running tokenizer on validation dataset #1: 100%|██████████| 1/1 [00:00<00:00,  2.35ba/s][ARunning tokenizer on validation dataset #1: 100%|██████████| 1/1 [00:00<00:00,  2.35ba/s]

Running tokenizer on validation dataset #2: 100%|██████████| 1/1 [00:00<00:00,  2.46ba/s][A[ARunning tokenizer on validation dataset #2: 100%|██████████| 1/1 [00:00<00:00,  2.45ba/s]




Running tokenizer on validation dataset #5: 100%|██████████| 1/1 [00:00<00:00,  2.54ba/s][A[A[A[A[ARunning tokenizer on validation dataset #5: 100%|██████████| 1/1 [00:00<00:00,  2.54ba/s]



Running tokenizer on validation dataset #4: 100%|██████████| 1/1 [00:00<00:00,  2.21ba/s][A[A[A[ARunning tokenizer on validation dataset #4: 100%|██████████| 1/1 [00:00<00:00,  2.21ba/s]





Running tokenizer on validation dataset #6: 100%|██████████| 1/1 [00:00<00:00,  2.60ba/s][A[A[A[A[A[ARunning tokenizer on validation dataset #6: 100%|██████████| 1/1 [00:00<00:00,  2.60ba/s]


Running tokenizer on validation dataset #3: 100%|██████████| 1/1 [00:00<00:00,  1.88ba/s][A[A[ARunning tokenizer on validation dataset #3: 100%|██████████| 1/1 [00:00<00:00,  1.88ba/s]






Running tokenizer on validation dataset #7: 100%|██████████| 1/1 [00:00<00:00,  2.57ba/s][A[A[A[A[A[A[ARunning tokenizer on validation dataset #7: 100%|██████████| 1/1 [00:00<00:00,  2.56ba/s]







Running tokenizer on validation dataset #8: 100%|██████████| 1/1 [00:00<00:00,  2.35ba/s][A[A[A[A[A[A[A[ARunning tokenizer on validation dataset #8: 100%|██████████| 1/1 [00:00<00:00,  2.35ba/s]








Running tokenizer on validation dataset #9: 100%|██████████| 1/1 [00:00<00:00,  2.41ba/s][A[A[A[A[A[A[A[A[ARunning tokenizer on validation dataset #9: 100%|██████████| 1/1 [00:00<00:00,  2.41ba/s]Running tokenizer on validation dataset #0: 100%|██████████| 1/1 [00:01<00:00,  1.36s/ba]Running tokenizer on validation dataset #0: 100%|██████████| 1/1 [00:01<00:00,  1.36s/ba]








/afs/crc.nd.edu/user/m/myu2/anaconda2/envs/bert/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
[INFO|trainer.py:1279] 2022-10-19 18:51:47,402 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-10-19 18:51:47,402 >>   Num examples = 2000
[INFO|trainer.py:1281] 2022-10-19 18:51:47,402 >>   Num Epochs = 2
[INFO|trainer.py:1282] 2022-10-19 18:51:47,402 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1283] 2022-10-19 18:51:47,403 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1284] 2022-10-19 18:51:47,403 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1285] 2022-10-19 18:51:47,403 >>   Total optimization steps = 250
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [00:00<03:21,  1.24it/s]  1%|          | 2/250 [00:01<03:08,  1.32it/s]  1%|          | 3/250 [00:02<03:03,  1.34it/s]  2%|▏         | 4/250 [00:02<03:01,  1.36it/s]  2%|▏         | 5/250 [00:03<02:59,  1.36it/s]  2%|▏         | 6/250 [00:04<02:58,  1.37it/s]  3%|▎         | 7/250 [00:05<02:57,  1.37it/s]  3%|▎         | 8/250 [00:05<02:56,  1.37it/s]  4%|▎         | 9/250 [00:06<02:55,  1.38it/s]  4%|▍         | 10/250 [00:07<02:54,  1.38it/s]  4%|▍         | 11/250 [00:08<02:53,  1.38it/s]  5%|▍         | 12/250 [00:08<02:52,  1.38it/s]  5%|▌         | 13/250 [00:09<02:52,  1.38it/s]  6%|▌         | 14/250 [00:10<02:51,  1.38it/s]  6%|▌         | 15/250 [00:10<02:50,  1.38it/s]  6%|▋         | 16/250 [00:11<02:49,  1.38it/s]  7%|▋         | 17/250 [00:12<02:49,  1.38it/s]  7%|▋         | 18/250 [00:13<02:48,  1.38it/s]  8%|▊         | 19/250 [00:13<02:48,  1.37it/s]  8%|▊         | 20/250 [00:14<02:47,  1.37it/s]  8%|▊         | 21/250 [00:15<02:46,  1.37it/s]  9%|▉         | 22/250 [00:16<02:45,  1.37it/s]  9%|▉         | 23/250 [00:16<02:45,  1.37it/s] 10%|▉         | 24/250 [00:17<02:44,  1.37it/s] 10%|█         | 25/250 [00:18<02:44,  1.37it/s] 10%|█         | 26/250 [00:18<02:43,  1.37it/s] 11%|█         | 27/250 [00:19<02:42,  1.37it/s] 11%|█         | 28/250 [00:20<02:42,  1.37it/s] 12%|█▏        | 29/250 [00:21<02:41,  1.37it/s] 12%|█▏        | 30/250 [00:21<02:40,  1.37it/s] 12%|█▏        | 31/250 [00:22<02:40,  1.37it/s] 13%|█▎        | 32/250 [00:23<02:39,  1.37it/s] 13%|█▎        | 33/250 [00:24<02:38,  1.37it/s] 14%|█▎        | 34/250 [00:24<02:38,  1.36it/s] 14%|█▍        | 35/250 [00:25<02:37,  1.36it/s] 14%|█▍        | 36/250 [00:26<02:36,  1.36it/s] 15%|█▍        | 37/250 [00:27<02:36,  1.36it/s] 15%|█▌        | 38/250 [00:27<02:35,  1.36it/s] 16%|█▌        | 39/250 [00:28<02:34,  1.37it/s] 16%|█▌        | 40/250 [00:29<02:33,  1.36it/s] 16%|█▋        | 41/250 [00:29<02:33,  1.36it/s] 17%|█▋        | 42/250 [00:30<02:32,  1.36it/s] 17%|█▋        | 43/250 [00:31<02:31,  1.36it/s] 18%|█▊        | 44/250 [00:32<02:31,  1.36it/s] 18%|█▊        | 45/250 [00:32<02:30,  1.36it/s] 18%|█▊        | 46/250 [00:33<02:29,  1.36it/s] 19%|█▉        | 47/250 [00:34<02:28,  1.36it/s] 19%|█▉        | 48/250 [00:35<02:28,  1.36it/s] 20%|█▉        | 49/250 [00:35<02:27,  1.36it/s] 20%|██        | 50/250 [00:36<02:27,  1.36it/s] 20%|██        | 51/250 [00:37<02:26,  1.36it/s] 21%|██        | 52/250 [00:38<02:25,  1.36it/s] 21%|██        | 53/250 [00:38<02:25,  1.36it/s] 22%|██▏       | 54/250 [00:39<02:24,  1.36it/s] 22%|██▏       | 55/250 [00:40<02:23,  1.36it/s] 22%|██▏       | 56/250 [00:41<02:23,  1.35it/s] 23%|██▎       | 57/250 [00:41<02:22,  1.36it/s] 23%|██▎       | 58/250 [00:42<02:21,  1.36it/s] 24%|██▎       | 59/250 [00:43<02:20,  1.36it/s] 24%|██▍       | 60/250 [00:43<02:19,  1.36it/s] 24%|██▍       | 61/250 [00:44<02:19,  1.36it/s] 25%|██▍       | 62/250 [00:45<02:18,  1.36it/s] 25%|██▌       | 63/250 [00:46<02:18,  1.35it/s] 26%|██▌       | 64/250 [00:46<02:17,  1.35it/s] 26%|██▌       | 65/250 [00:47<02:32,  1.21it/s] 26%|██▋       | 66/250 [00:48<02:27,  1.25it/s] 27%|██▋       | 67/250 [00:49<02:23,  1.28it/s] 27%|██▋       | 68/250 [00:50<02:20,  1.30it/s] 28%|██▊       | 69/250 [00:50<02:17,  1.31it/s] 28%|██▊       | 70/250 [00:51<02:15,  1.32it/s] 28%|██▊       | 71/250 [00:52<02:14,  1.33it/s] 29%|██▉       | 72/250 [00:53<02:13,  1.34it/s] 29%|██▉       | 73/250 [00:53<02:12,  1.34it/s] 30%|██▉       | 74/250 [00:54<02:11,  1.34it/s] 30%|███       | 75/250 [00:55<02:10,  1.34it/s] 30%|███       | 76/250 [00:56<02:09,  1.34it/s] 31%|███       | 77/250 [00:56<02:08,  1.35it/s] 31%|███       | 78/250 [00:57<02:07,  1.35it/s] 32%|███▏      | 79/250 [00:58<02:06,  1.35it/s] 32%|███▏      | 80/250 [00:59<02:06,  1.35it/s] 32%|███▏      | 81/250 [00:59<02:05,  1.35it/s] 33%|███▎      | 82/250 [01:00<02:04,  1.35it/s] 33%|███▎      | 83/250 [01:01<02:04,  1.35it/s] 34%|███▎      | 84/250 [01:02<02:03,  1.35it/s] 34%|███▍      | 85/250 [01:02<02:02,  1.34it/s] 34%|███▍      | 86/250 [01:03<02:02,  1.34it/s] 35%|███▍      | 87/250 [01:04<02:01,  1.34it/s] 35%|███▌      | 88/250 [01:05<02:00,  1.34it/s] 36%|███▌      | 89/250 [01:05<01:59,  1.34it/s] 36%|███▌      | 90/250 [01:06<01:59,  1.34it/s] 36%|███▋      | 91/250 [01:07<01:58,  1.34it/s] 37%|███▋      | 92/250 [01:08<01:57,  1.34it/s] 37%|███▋      | 93/250 [01:08<01:56,  1.34it/s] 38%|███▊      | 94/250 [01:09<01:56,  1.34it/s] 38%|███▊      | 95/250 [01:10<01:55,  1.34it/s] 38%|███▊      | 96/250 [01:10<01:54,  1.34it/s] 39%|███▉      | 97/250 [01:11<01:53,  1.34it/s] 39%|███▉      | 98/250 [01:12<01:53,  1.34it/s] 40%|███▉      | 99/250 [01:13<01:52,  1.34it/s] 40%|████      | 100/250 [01:13<01:51,  1.34it/s] 40%|████      | 101/250 [01:14<01:51,  1.34it/s] 41%|████      | 102/250 [01:15<01:50,  1.34it/s] 41%|████      | 103/250 [01:16<01:49,  1.34it/s] 42%|████▏     | 104/250 [01:16<01:48,  1.34it/s] 42%|████▏     | 105/250 [01:17<01:48,  1.34it/s] 42%|████▏     | 106/250 [01:18<01:47,  1.34it/s] 43%|████▎     | 107/250 [01:19<01:46,  1.34it/s] 43%|████▎     | 108/250 [01:19<01:46,  1.34it/s] 44%|████▎     | 109/250 [01:20<01:45,  1.34it/s] 44%|████▍     | 110/250 [01:21<01:44,  1.34it/s] 44%|████▍     | 111/250 [01:22<01:43,  1.34it/s] 45%|████▍     | 112/250 [01:22<01:43,  1.34it/s] 45%|████▌     | 113/250 [01:23<01:42,  1.34it/s] 46%|████▌     | 114/250 [01:24<01:41,  1.34it/s] 46%|████▌     | 115/250 [01:25<01:40,  1.34it/s] 46%|████▋     | 116/250 [01:25<01:40,  1.34it/s] 47%|████▋     | 117/250 [01:26<01:39,  1.34it/s] 47%|████▋     | 118/250 [01:27<01:38,  1.34it/s] 48%|████▊     | 119/250 [01:28<01:37,  1.34it/s] 48%|████▊     | 120/250 [01:28<01:37,  1.34it/s] 48%|████▊     | 121/250 [01:29<01:36,  1.34it/s] 49%|████▉     | 122/250 [01:30<01:35,  1.34it/s] 49%|████▉     | 123/250 [01:31<01:34,  1.34it/s] 50%|████▉     | 124/250 [01:31<01:34,  1.34it/s] 50%|█████     | 125/250 [01:32<01:33,  1.34it/s][INFO|trainer.py:571] 2022-10-19 18:53:20,059 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.
[INFO|trainer.py:2389] 2022-10-19 18:53:20,066 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-10-19 18:53:20,066 >>   Num examples = 200
[INFO|trainer.py:2394] 2022-10-19 18:53:20,066 >>   Batch size = 8

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:01<00:18,  1.27it/s][A
 12%|█▏        | 3/25 [00:03<00:24,  1.12s/it][A
 16%|█▌        | 4/25 [00:04<00:27,  1.29s/it][A
 20%|██        | 5/25 [00:06<00:27,  1.39s/it][A
 24%|██▍       | 6/25 [00:07<00:27,  1.45s/it][A
 28%|██▊       | 7/25 [00:09<00:26,  1.49s/it][A
 32%|███▏      | 8/25 [00:10<00:25,  1.51s/it][A
 36%|███▌      | 9/25 [00:12<00:24,  1.55s/it][A
 40%|████      | 10/25 [00:14<00:23,  1.55s/it][A
 44%|████▍     | 11/25 [00:15<00:21,  1.56s/it][A
 48%|████▊     | 12/25 [00:17<00:20,  1.56s/it][A
 52%|█████▏    | 13/25 [00:18<00:18,  1.57s/it][A
 56%|█████▌    | 14/25 [00:20<00:17,  1.57s/it][A
 60%|██████    | 15/25 [00:22<00:15,  1.57s/it][A
 64%|██████▍   | 16/25 [00:23<00:14,  1.57s/it][A
 68%|██████▊   | 17/25 [00:25<00:12,  1.57s/it][A
 72%|███████▏  | 18/25 [00:26<00:11,  1.60s/it][A
 76%|███████▌  | 19/25 [00:28<00:09,  1.59s/it][A
 80%|████████  | 20/25 [00:29<00:07,  1.58s/it][A
 84%|████████▍ | 21/25 [00:31<00:06,  1.58s/it][A
 88%|████████▊ | 22/25 [00:33<00:04,  1.57s/it][A
 92%|█████████▏| 23/25 [00:34<00:03,  1.57s/it][A
 96%|█████████▌| 24/25 [00:36<00:01,  1.57s/it][A
100%|██████████| 25/25 [00:37<00:00,  1.57s/it][A                                                 
                                               [A 50%|█████     | 125/250 [02:23<01:33,  1.34it/s]
100%|██████████| 25/25 [00:49<00:00,  1.57s/it][A
                                               [A[INFO|trainer.py:2139] 2022-10-19 18:54:11,405 >> Saving model checkpoint to /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-125
[INFO|configuration_utils.py:439] 2022-10-19 18:54:11,407 >> Configuration saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-125/config.json
[INFO|modeling_utils.py:1084] 2022-10-19 18:54:15,858 >> Model weights saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:2094] 2022-10-19 18:54:15,859 >> tokenizer config file saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2022-10-19 18:54:15,860 >> Special tokens file saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-125/special_tokens_map.json
[INFO|trainer.py:2217] 2022-10-19 18:54:25,819 >> Deleting older checkpoint [/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-1] due to args.save_total_limit
 50%|█████     | 126/250 [02:39<42:30, 20.57s/it] 51%|█████     | 127/250 [02:40<29:56, 14.61s/it] 51%|█████     | 128/250 [02:40<21:13, 10.44s/it] 52%|█████▏    | 129/250 [02:41<15:09,  7.52s/it] 52%|█████▏    | 130/250 [02:42<10:56,  5.47s/it] 52%|█████▏    | 131/250 [02:42<08:01,  4.04s/it] 53%|█████▎    | 132/250 [02:43<05:58,  3.04s/it] 53%|█████▎    | 133/250 [02:44<04:33,  2.34s/it] 54%|█████▎    | 134/250 [02:45<03:34,  1.85s/it] 54%|█████▍    | 135/250 [02:45<02:53,  1.51s/it] 54%|█████▍    | 136/250 [02:46<02:24,  1.27s/it] 55%|█████▍    | 137/250 [02:47<02:04,  1.10s/it] 55%|█████▌    | 138/250 [02:47<01:49,  1.02it/s] 56%|█████▌    | 139/250 [02:48<01:39,  1.11it/s] 56%|█████▌    | 140/250 [02:49<01:32,  1.19it/s] 56%|█████▋    | 141/250 [02:50<01:27,  1.25it/s] 57%|█████▋    | 142/250 [02:50<01:23,  1.30it/s] 57%|█████▋    | 143/250 [02:51<01:20,  1.33it/s] 58%|█████▊    | 144/250 [02:52<01:18,  1.35it/s] 58%|█████▊    | 145/250 [02:52<01:16,  1.37it/s] 58%|█████▊    | 146/250 [02:53<01:15,  1.38it/s] 59%|█████▉    | 147/250 [02:54<01:14,  1.39it/s] 59%|█████▉    | 148/250 [02:55<01:13,  1.39it/s] 60%|█████▉    | 149/250 [02:55<01:12,  1.40it/s] 60%|██████    | 150/250 [02:56<01:11,  1.40it/s] 60%|██████    | 151/250 [02:57<01:10,  1.41it/s] 61%|██████    | 152/250 [02:57<01:09,  1.41it/s] 61%|██████    | 153/250 [02:58<01:08,  1.41it/s] 62%|██████▏   | 154/250 [02:59<01:08,  1.41it/s] 62%|██████▏   | 155/250 [02:59<01:07,  1.41it/s] 62%|██████▏   | 156/250 [03:00<01:06,  1.41it/s] 63%|██████▎   | 157/250 [03:01<01:06,  1.41it/s] 63%|██████▎   | 158/250 [03:02<01:05,  1.41it/s] 64%|██████▎   | 159/250 [03:02<01:04,  1.41it/s] 64%|██████▍   | 160/250 [03:03<01:04,  1.41it/s] 64%|██████▍   | 161/250 [03:04<01:03,  1.41it/s] 65%|██████▍   | 162/250 [03:04<01:02,  1.40it/s] 65%|██████▌   | 163/250 [03:05<01:01,  1.41it/s] 66%|██████▌   | 164/250 [03:06<01:01,  1.41it/s] 66%|██████▌   | 165/250 [03:07<01:00,  1.41it/s] 66%|██████▋   | 166/250 [03:07<00:59,  1.41it/s] 67%|██████▋   | 167/250 [03:08<00:59,  1.41it/s] 67%|██████▋   | 168/250 [03:09<00:58,  1.41it/s] 68%|██████▊   | 169/250 [03:09<00:57,  1.41it/s] 68%|██████▊   | 170/250 [03:10<00:56,  1.40it/s] 68%|██████▊   | 171/250 [03:11<00:56,  1.40it/s] 69%|██████▉   | 172/250 [03:12<00:55,  1.40it/s] 69%|██████▉   | 173/250 [03:12<00:54,  1.40it/s] 70%|██████▉   | 174/250 [03:13<00:54,  1.40it/s] 70%|███████   | 175/250 [03:14<00:53,  1.40it/s] 70%|███████   | 176/250 [03:14<00:52,  1.40it/s] 71%|███████   | 177/250 [03:15<00:52,  1.40it/s] 71%|███████   | 178/250 [03:16<00:56,  1.28it/s] 72%|███████▏  | 179/250 [03:17<00:53,  1.32it/s] 72%|███████▏  | 180/250 [03:17<00:52,  1.34it/s] 72%|███████▏  | 181/250 [03:18<00:50,  1.35it/s] 73%|███████▎  | 182/250 [03:19<00:49,  1.37it/s] 73%|███████▎  | 183/250 [03:20<00:48,  1.38it/s] 74%|███████▎  | 184/250 [03:20<00:47,  1.38it/s] 74%|███████▍  | 185/250 [03:21<00:46,  1.39it/s] 74%|███████▍  | 186/250 [03:22<00:45,  1.39it/s] 75%|███████▍  | 187/250 [03:23<00:45,  1.39it/s] 75%|███████▌  | 188/250 [03:23<00:44,  1.39it/s] 76%|███████▌  | 189/250 [03:24<00:43,  1.40it/s] 76%|███████▌  | 190/250 [03:25<00:42,  1.40it/s] 76%|███████▋  | 191/250 [03:25<00:42,  1.40it/s] 77%|███████▋  | 192/250 [03:26<00:41,  1.40it/s] 77%|███████▋  | 193/250 [03:27<00:40,  1.40it/s] 78%|███████▊  | 194/250 [03:28<00:40,  1.39it/s] 78%|███████▊  | 195/250 [03:28<00:39,  1.39it/s] 78%|███████▊  | 196/250 [03:29<00:38,  1.39it/s] 79%|███████▉  | 197/250 [03:30<00:38,  1.39it/s] 79%|███████▉  | 198/250 [03:30<00:37,  1.40it/s] 80%|███████▉  | 199/250 [03:31<00:36,  1.40it/s] 80%|████████  | 200/250 [03:32<00:35,  1.39it/s] 80%|████████  | 201/250 [03:33<00:35,  1.39it/s] 81%|████████  | 202/250 [03:33<00:34,  1.40it/s] 81%|████████  | 203/250 [03:34<00:33,  1.40it/s] 82%|████████▏ | 204/250 [03:35<00:32,  1.40it/s] 82%|████████▏ | 205/250 [03:35<00:32,  1.39it/s] 82%|████████▏ | 206/250 [03:36<00:31,  1.40it/s] 83%|████████▎ | 207/250 [03:37<00:30,  1.39it/s] 83%|████████▎ | 208/250 [03:38<00:30,  1.39it/s] 84%|████████▎ | 209/250 [03:38<00:29,  1.39it/s] 84%|████████▍ | 210/250 [03:39<00:28,  1.39it/s] 84%|████████▍ | 211/250 [03:40<00:27,  1.39it/s] 85%|████████▍ | 212/250 [03:40<00:27,  1.39it/s] 85%|████████▌ | 213/250 [03:41<00:26,  1.39it/s] 86%|████████▌ | 214/250 [03:42<00:25,  1.39it/s] 86%|████████▌ | 215/250 [03:43<00:25,  1.39it/s] 86%|████████▋ | 216/250 [03:43<00:24,  1.39it/s] 87%|████████▋ | 217/250 [03:44<00:23,  1.39it/s] 87%|████████▋ | 218/250 [03:45<00:22,  1.39it/s] 88%|████████▊ | 219/250 [03:45<00:22,  1.39it/s] 88%|████████▊ | 220/250 [03:46<00:21,  1.39it/s] 88%|████████▊ | 221/250 [03:47<00:20,  1.39it/s] 89%|████████▉ | 222/250 [03:48<00:20,  1.39it/s] 89%|████████▉ | 223/250 [03:48<00:19,  1.39it/s] 90%|████████▉ | 224/250 [03:49<00:18,  1.39it/s] 90%|█████████ | 225/250 [03:50<00:17,  1.39it/s] 90%|█████████ | 226/250 [03:50<00:17,  1.39it/s] 91%|█████████ | 227/250 [03:51<00:16,  1.39it/s] 91%|█████████ | 228/250 [03:52<00:15,  1.39it/s] 92%|█████████▏| 229/250 [03:53<00:15,  1.39it/s] 92%|█████████▏| 230/250 [03:53<00:14,  1.39it/s] 92%|█████████▏| 231/250 [03:54<00:13,  1.39it/s] 93%|█████████▎| 232/250 [03:55<00:12,  1.39it/s] 93%|█████████▎| 233/250 [03:56<00:12,  1.39it/s] 94%|█████████▎| 234/250 [03:56<00:11,  1.39it/s] 94%|█████████▍| 235/250 [03:57<00:10,  1.39it/s] 94%|█████████▍| 236/250 [03:58<00:10,  1.39it/s] 95%|█████████▍| 237/250 [03:58<00:09,  1.39it/s] 95%|█████████▌| 238/250 [03:59<00:08,  1.39it/s] 96%|█████████▌| 239/250 [04:00<00:07,  1.39it/s] 96%|█████████▌| 240/250 [04:01<00:07,  1.39it/s] 96%|█████████▋| 241/250 [04:01<00:06,  1.39it/s] 97%|█████████▋| 242/250 [04:02<00:05,  1.39it/s] 97%|█████████▋| 243/250 [04:03<00:05,  1.39it/s] 98%|█████████▊| 244/250 [04:03<00:04,  1.39it/s] 98%|█████████▊| 245/250 [04:04<00:03,  1.39it/s] 98%|█████████▊| 246/250 [04:05<00:02,  1.39it/s] 99%|█████████▉| 247/250 [04:06<00:02,  1.39it/s] 99%|█████████▉| 248/250 [04:06<00:01,  1.39it/s]100%|█████████▉| 249/250 [04:07<00:00,  1.39it/s]100%|██████████| 250/250 [04:08<00:00,  1.39it/s][INFO|trainer.py:571] 2022-10-19 18:55:55,687 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.
[INFO|trainer.py:2389] 2022-10-19 18:55:55,693 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-10-19 18:55:55,693 >>   Num examples = 200
[INFO|trainer.py:2394] 2022-10-19 18:55:55,693 >>   Batch size = 8
{'eval_rouge2': 22.8, 'eval_rougeL': 29.61, 'eval_gen_len': 42.93, 'epoch': 1.0}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:01<00:14,  1.58it/s][A
 12%|█▏        | 3/25 [00:02<00:19,  1.11it/s][A
 16%|█▌        | 4/25 [00:03<00:22,  1.07s/it][A
 20%|██        | 5/25 [00:05<00:22,  1.13s/it][A
 24%|██▍       | 6/25 [00:06<00:22,  1.18s/it][A
 28%|██▊       | 7/25 [00:07<00:21,  1.20s/it][A
 32%|███▏      | 8/25 [00:08<00:20,  1.22s/it][A
 36%|███▌      | 9/25 [00:10<00:19,  1.23s/it][A
 40%|████      | 10/25 [00:11<00:18,  1.24s/it][A
 44%|████▍     | 11/25 [00:12<00:17,  1.24s/it][A
 48%|████▊     | 12/25 [00:13<00:16,  1.25s/it][A
 52%|█████▏    | 13/25 [00:15<00:15,  1.25s/it][A
 56%|█████▌    | 14/25 [00:16<00:14,  1.28s/it][A
 60%|██████    | 15/25 [00:17<00:12,  1.28s/it][A
 64%|██████▍   | 16/25 [00:19<00:11,  1.29s/it][A
 68%|██████▊   | 17/25 [00:20<00:10,  1.29s/it][A
 72%|███████▏  | 18/25 [00:21<00:08,  1.29s/it][A
 76%|███████▌  | 19/25 [00:22<00:07,  1.28s/it][A
 80%|████████  | 20/25 [00:24<00:06,  1.28s/it][A
 84%|████████▍ | 21/25 [00:25<00:05,  1.27s/it][A
 88%|████████▊ | 22/25 [00:26<00:03,  1.27s/it][A
 92%|█████████▏| 23/25 [00:28<00:02,  1.30s/it][A
 96%|█████████▌| 24/25 [00:29<00:01,  1.29s/it][A
100%|██████████| 25/25 [00:30<00:00,  1.29s/it][A                                                 
                                               [A100%|██████████| 250/250 [04:52<00:00,  1.39it/s]
100%|██████████| 25/25 [00:43<00:00,  1.29s/it][A
                                               [A[INFO|trainer.py:2139] 2022-10-19 18:56:40,368 >> Saving model checkpoint to /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-250
[INFO|configuration_utils.py:439] 2022-10-19 18:56:40,371 >> Configuration saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-250/config.json
[INFO|modeling_utils.py:1084] 2022-10-19 18:56:44,786 >> Model weights saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2094] 2022-10-19 18:56:44,787 >> tokenizer config file saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2022-10-19 18:56:44,788 >> Special tokens file saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-250/special_tokens_map.json
[INFO|trainer.py:2217] 2022-10-19 18:56:53,291 >> Deleting older checkpoint [/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/checkpoint-2] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-10-19 18:56:53,592 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 100%|██████████| 250/250 [05:06<00:00,  1.39it/s]100%|██████████| 250/250 [05:06<00:00,  1.22s/it]
[INFO|trainer.py:2139] 2022-10-19 18:56:53,594 >> Saving model checkpoint to /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019
[INFO|configuration_utils.py:439] 2022-10-19 18:56:53,596 >> Configuration saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/config.json
[INFO|modeling_utils.py:1084] 2022-10-19 18:56:58,165 >> Model weights saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/pytorch_model.bin
[INFO|tokenization_utils_base.py:2094] 2022-10-19 18:56:58,168 >> tokenizer config file saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/tokenizer_config.json
[INFO|tokenization_utils_base.py:2100] 2022-10-19 18:56:58,169 >> Special tokens file saved in /afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/seq2seq/bart_input768_output64_epoch3_lr3e-5_bsz4_2k500_1019/special_tokens_map.json
{'eval_rouge2': 22.45, 'eval_rougeL': 29.47, 'eval_gen_len': 43.175, 'epoch': 2.0}
{'train_runtime': 306.19, 'train_samples_per_second': 13.064, 'train_steps_per_second': 0.816, 'train_loss': 1.8000732421875, 'epoch': 2.0}
***** train metrics *****
  epoch                    =        2.0
  train_loss               =     1.8001
  train_runtime            = 0:05:06.18
  train_samples            =       2000
  train_samples_per_second =     13.064
  train_steps_per_second   =      0.816
10/19/2022 18:56:58 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:571] 2022-10-19 18:56:58,699 >> The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.
[INFO|trainer.py:2389] 2022-10-19 18:56:58,706 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-10-19 18:56:58,706 >>   Num examples = 200
[INFO|trainer.py:2394] 2022-10-19 18:56:58,706 >>   Batch size = 8
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:01<00:14,  1.58it/s] 12%|█▏        | 3/25 [00:02<00:19,  1.11it/s] 16%|█▌        | 4/25 [00:03<00:21,  1.03s/it] 20%|██        | 5/25 [00:07<00:38,  1.93s/it] 24%|██▍       | 6/25 [00:08<00:32,  1.71s/it] 28%|██▊       | 7/25 [00:09<00:28,  1.57s/it] 32%|███▏      | 8/25 [00:11<00:24,  1.46s/it] 36%|███▌      | 9/25 [00:12<00:22,  1.43s/it] 40%|████      | 10/25 [00:13<00:20,  1.37s/it] 44%|████▍     | 11/25 [00:15<00:18,  1.34s/it] 48%|████▊     | 12/25 [00:16<00:17,  1.31s/it] 52%|█████▏    | 13/25 [00:17<00:15,  1.30s/it] 56%|█████▌    | 14/25 [00:18<00:14,  1.29s/it] 60%|██████    | 15/25 [00:20<00:12,  1.28s/it] 64%|██████▍   | 16/25 [00:21<00:11,  1.27s/it] 68%|██████▊   | 17/25 [00:22<00:10,  1.27s/it] 72%|███████▏  | 18/25 [00:23<00:09,  1.29s/it] 76%|███████▌  | 19/25 [00:25<00:07,  1.28s/it] 80%|████████  | 20/25 [00:26<00:06,  1.27s/it] 84%|████████▍ | 21/25 [00:27<00:05,  1.27s/it] 88%|████████▊ | 22/25 [00:28<00:03,  1.26s/it] 92%|█████████▏| 23/25 [00:30<00:02,  1.26s/it] 96%|█████████▌| 24/25 [00:31<00:01,  1.26s/it]100%|██████████| 25/25 [00:32<00:00,  1.26s/it]100%|██████████| 25/25 [00:45<00:00,  1.81s/it]
***** eval metrics *****
  epoch        =    2.0
  eval_gen_len = 43.175
  eval_rouge2  =  22.45
  eval_rougeL  =  29.47
  eval_samples =    200
