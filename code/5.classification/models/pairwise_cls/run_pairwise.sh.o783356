11/07/2022 23:13:11 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
11/07/2022 23:13:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pairwise_cls/cross_enc_1107/runs/Nov07_23-13-11_qa-2080ti-007.crc.nd.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=4.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pairwise_cls/cross_enc_1107,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/afs/crc.nd.edu/group/dmsquare/vol2/myu2/ComparisonSentences/experiments/pairwise_cls/cross_enc_1107,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/07/2022 23:13:11 - WARNING - datasets.builder - Using custom data configuration default-cf271586fdde3b4e
11/07/2022 23:13:11 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/07/2022 23:13:11 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-cf271586fdde3b4e/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
11/07/2022 23:13:12 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-cf271586fdde3b4e/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
11/07/2022 23:13:12 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-cf271586fdde3b4e/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
11/07/2022 23:13:17 - WARNING - datasets.builder - Using custom data configuration default-96b84d8bca05d054
11/07/2022 23:13:17 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/07/2022 23:13:17 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-96b84d8bca05d054/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
11/07/2022 23:13:17 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-96b84d8bca05d054/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
11/07/2022 23:13:17 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-96b84d8bca05d054/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
11/07/2022 23:13:17 - WARNING - datasets.builder - Using custom data configuration default-4663508d96eaf5eb
11/07/2022 23:13:17 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/07/2022 23:13:17 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-4663508d96eaf5eb/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
11/07/2022 23:13:17 - WARNING - datasets.builder - Reusing dataset json (/afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-4663508d96eaf5eb/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)
11/07/2022 23:13:17 - INFO - datasets.info - Loading Dataset info from /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-4663508d96eaf5eb/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253
[INFO|configuration_utils.py:648] 2022-11-07 23:13:18,565 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:684] 2022-11-07 23:13:18,567 >> Model config BertConfig {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "xnli",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:648] 2022-11-07 23:13:18,846 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:684] 2022-11-07 23:13:18,847 >> Model config BertConfig {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:1786] 2022-11-07 23:13:19,644 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791
[INFO|tokenization_utils_base.py:1786] 2022-11-07 23:13:19,645 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6
[INFO|tokenization_utils_base.py:1786] 2022-11-07 23:13:19,645 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-11-07 23:13:19,645 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-11-07 23:13:19,645 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|configuration_utils.py:648] 2022-11-07 23:13:19,774 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
[INFO|configuration_utils.py:684] 2022-11-07 23:13:19,775 >> Model config BertConfig {
  "_name_or_path": "bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:1431] 2022-11-07 23:13:20,985 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
[WARNING|modeling_utils.py:1694] 2022-11-07 23:13:27,290 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1705] 2022-11-07 23:13:27,290 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/07/2022 23:13:27 - INFO - datasets.arrow_dataset - Caching indices mapping at /afs/crc.nd.edu/user/m/myu2/.cache/huggingface/datasets/json/default-cf271586fdde3b4e/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-d9f75e8e9c7cc33f.arrow
Running tokenizer on train dataset #0:   0%|          | 0/356 [00:00<?, ?ba/s]
Running tokenizer on train dataset #1:   0%|          | 0/356 [00:00<?, ?ba/s][A

Running tokenizer on train dataset #2:   0%|          | 0/356 [00:00<?, ?ba/s][A[A


Running tokenizer on train dataset #3:   0%|          | 0/356 [00:00<?, ?ba/s][A[A[A

Running tokenizer on train dataset #2:   0%|          | 1/356 [00:00<02:16,  2.60ba/s][A[ARunning tokenizer on train dataset #0:   0%|          | 1/356 [00:00<02:24,  2.45ba/s]
Running tokenizer on train dataset #1:   0%|          | 1/356 [00:00<02:24,  2.46ba/s][A


Running tokenizer on train dataset #3:   0%|          | 1/356 [00:00<02:18,  2.57ba/s][A[A[A

Running tokenizer on train dataset #2:   1%|          | 2/356 [00:00<01:59,  2.96ba/s][A[A
Running tokenizer on train dataset #1:   1%|          | 2/356 [00:00<02:02,  2.89ba/s][ARunning tokenizer on train dataset #0:   1%|          | 2/356 [00:00<02:03,  2.87ba/s]


Running tokenizer on train dataset #3:   1%|          | 2/356 [00:00<02:00,  2.93ba/s][A[A[A

Running tokenizer on train dataset #2:   1%|          | 3/356 [00:00<01:52,  3.13ba/s][A[ARunning tokenizer on train dataset #0:   1%|          | 3/356 [00:01<01:54,  3.07ba/s]
Running tokenizer on train dataset #1:   1%|          | 3/356 [00:01<01:54,  3.07ba/s][A


Running tokenizer on train dataset #3:   1%|          | 3/356 [00:01<01:55,  3.06ba/s][A[A[A

Running tokenizer on train dataset #2:   1%|          | 4/356 [00:01<01:49,  3.20ba/s][A[ARunning tokenizer on train dataset #0:   1%|          | 4/356 [00:01<01:51,  3.15ba/s]
Running tokenizer on train dataset #1:   1%|          | 4/356 [00:01<01:51,  3.15ba/s][A


Running tokenizer on train dataset #3:   1%|          | 4/356 [00:01<01:52,  3.14ba/s][A[A[A

Running tokenizer on train dataset #2:   1%|▏         | 5/356 [00:01<01:47,  3.26ba/s][A[ARunning tokenizer on train dataset #0:   1%|▏         | 5/356 [00:01<01:49,  3.22ba/s]
Running tokenizer on train dataset #1:   1%|▏         | 5/356 [00:01<01:50,  3.19ba/s][A


Running tokenizer on train dataset #3:   1%|▏         | 5/356 [00:01<01:50,  3.19ba/s][A[A[A

Running tokenizer on train dataset #2:   2%|▏         | 6/356 [00:01<01:59,  2.92ba/s][A[ARunning tokenizer on train dataset #0:   2%|▏         | 6/356 [00:02<02:03,  2.84ba/s]
Running tokenizer on train dataset #1:   2%|▏         | 6/356 [00:02<02:03,  2.85ba/s][A


Running tokenizer on train dataset #3:   2%|▏         | 6/356 [00:02<02:05,  2.80ba/s][A[A[A

Running tokenizer on train dataset #2:   2%|▏         | 7/356 [00:02<01:54,  3.05ba/s][A[A
Running tokenizer on train dataset #1:   2%|▏         | 7/356 [00:02<01:56,  2.99ba/s][ARunning tokenizer on train dataset #0:   2%|▏         | 7/356 [00:02<01:57,  2.97ba/s]


Running tokenizer on train dataset #3:   2%|▏         | 7/356 [00:02<01:59,  2.92ba/s][A[A[A

Running tokenizer on train dataset #2:   2%|▏         | 8/356 [00:02<01:50,  3.15ba/s][A[A
Running tokenizer on train dataset #1:   2%|▏         | 8/356 [00:02<01:51,  3.11ba/s][ARunning tokenizer on train dataset #0:   2%|▏         | 8/356 [00:02<01:53,  3.07ba/s]


Running tokenizer on train dataset #3:   2%|▏         | 8/356 [00:02<01:59,  2.92ba/s][A[A[A

Running tokenizer on train dataset #2:   3%|▎         | 9/356 [00:02<01:48,  3.21ba/s][A[A
Running tokenizer on train dataset #1:   3%|▎         | 9/356 [00:02<01:49,  3.16ba/s][ARunning tokenizer on train dataset #0:   3%|▎         | 9/356 [00:03<01:55,  3.01ba/s]


Running tokenizer on train dataset #3:   3%|▎         | 9/356 [00:03<01:54,  3.03ba/s][A[A[A

Running tokenizer on train dataset #2:   3%|▎         | 10/356 [00:03<01:46,  3.25ba/s][A[A
Running tokenizer on train dataset #1:   3%|▎         | 10/356 [00:03<01:47,  3.21ba/s][ARunning tokenizer on train dataset #0:   3%|▎         | 10/356 [00:03<01:52,  3.07ba/s]


Running tokenizer on train dataset #3:   3%|▎         | 10/356 [00:03<01:52,  3.08ba/s][A[A[A

Running tokenizer on train dataset #2:   3%|▎         | 11/356 [00:03<01:45,  3.27ba/s][A[A
Running tokenizer on train dataset #1:   3%|▎         | 11/356 [00:03<01:45,  3.26ba/s][ARunning tokenizer on train dataset #0:   3%|▎         | 11/356 [00:03<01:49,  3.14ba/s]


Running tokenizer on train dataset #3:   3%|▎         | 11/356 [00:03<01:49,  3.16ba/s][A[A[A

Running tokenizer on train dataset #2:   3%|▎         | 12/356 [00:03<01:44,  3.30ba/s][A[A
Running tokenizer on train dataset #1:   3%|▎         | 12/356 [00:03<01:44,  3.29ba/s][ARunning tokenizer on train dataset #0:   3%|▎         | 12/356 [00:03<01:47,  3.20ba/s]


Running tokenizer on train dataset #3:   3%|▎         | 12/356 [00:03<01:49,  3.14ba/s][A[A[A

Running tokenizer on train dataset #2:   4%|▎         | 13/356 [00:04<01:43,  3.30ba/s][A[A
Running tokenizer on train dataset #1:   4%|▎         | 13/356 [00:04<01:43,  3.31ba/s][ARunning tokenizer on train dataset #0:   4%|▎         | 13/356 [00:04<01:45,  3.26ba/s]


Running tokenizer on train dataset #3:   4%|▎         | 13/356 [00:04<01:46,  3.21ba/s][A[A[A

Running tokenizer on train dataset #2:   4%|▍         | 14/356 [00:04<01:43,  3.32ba/s][A[A
Running tokenizer on train dataset #1:   4%|▍         | 14/356 [00:04<01:42,  3.34ba/s][ARunning tokenizer on train dataset #0:   4%|▍         | 14/356 [00:04<01:44,  3.27ba/s]


Running tokenizer on train dataset #3:   4%|▍         | 14/356 [00:04<01:44,  3.26ba/s][A[A[A

Running tokenizer on train dataset #2:   4%|▍         | 15/356 [00:04<01:42,  3.32ba/s][A[A
Running tokenizer on train dataset #1:   4%|▍         | 15/356 [00:04<01:41,  3.35ba/s][ARunning tokenizer on train dataset #0:   4%|▍         | 15/356 [00:04<01:43,  3.31ba/s]


Running tokenizer on train dataset #3:   4%|▍         | 15/356 [00:04<01:43,  3.30ba/s][A[A[A

Running tokenizer on train dataset #2:   4%|▍         | 16/356 [00:04<01:41,  3.34ba/s][A[A
Running tokenizer on train dataset #1:   4%|▍         | 16/356 [00:05<01:41,  3.35ba/s][ARunning tokenizer on train dataset #0:   4%|▍         | 16/356 [00:05<01:42,  3.33ba/s]


Running tokenizer on train dataset #3:   4%|▍         | 16/356 [00:05<01:42,  3.33ba/s][A[A[A

Running tokenizer on train dataset #2:   5%|▍         | 17/356 [00:05<01:48,  3.13ba/s][A[A
Running tokenizer on train dataset #1:   5%|▍         | 17/356 [00:05<01:47,  3.16ba/s][ARunning tokenizer on train dataset #0:   5%|▍         | 17/356 [00:05<01:52,  3.02ba/s]


Running tokenizer on train dataset #3:   5%|▍         | 17/356 [00:05<01:49,  3.09ba/s][A[A[A

Running tokenizer on train dataset #2:   5%|▌         | 18/356 [00:05<01:45,  3.19ba/s][A[A
Running tokenizer on train dataset #1:   5%|▌         | 18/356 [00:05<01:44,  3.23ba/s][ARunning tokenizer on train dataset #0:   5%|▌         | 18/356 [00:05<01:48,  3.11ba/s]


Running tokenizer on train dataset #3:   5%|▌         | 18/356 [00:05<01:46,  3.18ba/s][A[A[A

Running tokenizer on train dataset #2:   5%|▌         | 19/356 [00:05<01:43,  3.25ba/s][A[A
Running tokenizer on train dataset #1:   5%|▌         | 19/356 [00:05<01:42,  3.27ba/s][ARunning tokenizer on train dataset #0:   5%|▌         | 19/356 [00:06<01:45,  3.19ba/s]


Running tokenizer on train dataset #3:   5%|▌         | 19/356 [00:06<01:44,  3.23ba/s][A[A[A

Running tokenizer on train dataset #2:   6%|▌         | 20/356 [00:06<01:42,  3.29ba/s][A[A
Running tokenizer on train dataset #1:   6%|▌         | 20/356 [00:06<01:41,  3.30ba/s][ARunning tokenizer on train dataset #0:   6%|▌         | 20/356 [00:06<01:43,  3.24ba/s]


Running tokenizer on train dataset #3:   6%|▌         | 20/356 [00:06<01:43,  3.26ba/s][A[A[A

Running tokenizer on train dataset #2:   6%|▌         | 21/356 [00:06<01:41,  3.31ba/s][A[A
Running tokenizer on train dataset #1:   6%|▌         | 21/356 [00:06<01:40,  3.32ba/s][ARunning tokenizer on train dataset #0:   6%|▌         | 21/356 [00:06<01:41,  3.29ba/s]


Running tokenizer on train dataset #3:   6%|▌         | 21/356 [00:06<01:41,  3.31ba/s][A[A[A

Running tokenizer on train dataset #2:   6%|▌         | 22/356 [00:06<01:39,  3.34ba/s][A[A
Running tokenizer on train dataset #1:   6%|▌         | 22/356 [00:06<01:40,  3.32ba/s][ARunning tokenizer on train dataset #0:   6%|▌         | 22/356 [00:06<01:40,  3.32ba/s]


Running tokenizer on train dataset #3:   6%|▌         | 22/356 [00:06<01:40,  3.34ba/s][A[A[A

Running tokenizer on train dataset #2:   6%|▋         | 23/356 [00:07<01:39,  3.33ba/s][A[A
Running tokenizer on train dataset #1:   6%|▋         | 23/356 [00:07<01:39,  3.34ba/s][ARunning tokenizer on train dataset #0:   6%|▋         | 23/356 [00:07<01:39,  3.34ba/s]


Running tokenizer on train dataset #3:   6%|▋         | 23/356 [00:07<01:39,  3.35ba/s][A[A[A

Running tokenizer on train dataset #2:   7%|▋         | 24/356 [00:07<01:39,  3.34ba/s][A[A
Running tokenizer on train dataset #1:   7%|▋         | 24/356 [00:07<01:39,  3.35ba/s][ARunning tokenizer on train dataset #0:   7%|▋         | 24/356 [00:07<01:38,  3.36ba/s]


Running tokenizer on train dataset #3:   7%|▋         | 24/356 [00:07<01:38,  3.36ba/s][A[A[A

Running tokenizer on train dataset #2:   7%|▋         | 25/356 [00:07<01:39,  3.34ba/s][A[A
Running tokenizer on train dataset #1:   7%|▋         | 25/356 [00:07<01:38,  3.35ba/s][ARunning tokenizer on train dataset #0:   7%|▋         | 25/356 [00:07<01:37,  3.38ba/s]


Running tokenizer on train dataset #3:   7%|▋         | 25/356 [00:07<01:38,  3.38ba/s][A[A[A

Running tokenizer on train dataset #2:   7%|▋         | 26/356 [00:08<01:38,  3.34ba/s][A[A
Running tokenizer on train dataset #1:   7%|▋         | 26/356 [00:08<01:38,  3.36ba/s][ARunning tokenizer on train dataset #0:   7%|▋         | 26/356 [00:08<01:37,  3.38ba/s]


Running tokenizer on train dataset #3:   7%|▋         | 26/356 [00:08<01:38,  3.34ba/s][A[A[A

Running tokenizer on train dataset #2:   8%|▊         | 27/356 [00:08<01:38,  3.35ba/s][A[A
Running tokenizer on train dataset #1:   8%|▊         | 27/356 [00:08<01:37,  3.36ba/s][ARunning tokenizer on train dataset #0:   8%|▊         | 27/356 [00:08<01:37,  3.37ba/s]


Running tokenizer on train dataset #3:   8%|▊         | 27/356 [00:08<01:37,  3.36ba/s][A[A[A

Running tokenizer on train dataset #2:   8%|▊         | 28/356 [00:08<01:44,  3.15ba/s][A[A
Running tokenizer on train dataset #1:   8%|▊         | 28/356 [00:08<01:43,  3.18ba/s][ARunning tokenizer on train dataset #0:   8%|▊         | 28/356 [00:08<01:43,  3.18ba/s]


Running tokenizer on train dataset #3:   8%|▊         | 28/356 [00:08<01:43,  3.16ba/s][A[A[A

Running tokenizer on train dataset #2:   8%|▊         | 29/356 [00:08<01:41,  3.22ba/s][A[A
Running tokenizer on train dataset #1:   8%|▊         | 29/356 [00:09<01:41,  3.23ba/s][ARunning tokenizer on train dataset #0:   8%|▊         | 29/356 [00:09<01:41,  3.23ba/s]


Running tokenizer on train dataset #3:   8%|▊         | 29/356 [00:09<01:41,  3.22ba/s][A[A[A

Running tokenizer on train dataset #2:   8%|▊         | 30/356 [00:09<01:39,  3.27ba/s][A[A
Running tokenizer on train dataset #1:   8%|▊         | 30/356 [00:09<01:41,  3.21ba/s][ARunning tokenizer on train dataset #0:   8%|▊         | 30/356 [00:09<01:39,  3.28ba/s]


Running tokenizer on train dataset #3:   8%|▊         | 30/356 [00:09<01:40,  3.26ba/s][A[A[A

Running tokenizer on train dataset #2:   9%|▊         | 31/356 [00:09<01:38,  3.29ba/s][A[A
Running tokenizer on train dataset #1:   9%|▊         | 31/356 [00:09<01:39,  3.25ba/s][ARunning tokenizer on train dataset #0:   9%|▊         | 31/356 [00:09<01:38,  3.29ba/s]


Running tokenizer on train dataset #3:   9%|▊         | 31/356 [00:09<01:38,  3.31ba/s][A[A[A

Running tokenizer on train dataset #2:   9%|▉         | 32/356 [00:09<01:38,  3.30ba/s][A[A
Running tokenizer on train dataset #1:   9%|▉         | 32/356 [00:09<01:38,  3.30ba/s][ARunning tokenizer on train dataset #0:   9%|▉         | 32/356 [00:10<01:37,  3.32ba/s]


Running tokenizer on train dataset #3:   9%|▉         | 32/356 [00:10<01:38,  3.30ba/s][A[A[A

Running tokenizer on train dataset #2:   9%|▉         | 33/356 [00:10<01:37,  3.32ba/s][A[A
Running tokenizer on train dataset #1:   9%|▉         | 33/356 [00:10<01:37,  3.32ba/s][ARunning tokenizer on train dataset #0:   9%|▉         | 33/356 [00:10<01:36,  3.34ba/s]


Running tokenizer on train dataset #3:   9%|▉         | 33/356 [00:10<01:36,  3.34ba/s][A[A[A

Running tokenizer on train dataset #2:  10%|▉         | 34/356 [00:10<01:36,  3.33ba/s][A[A
Running tokenizer on train dataset #1:  10%|▉         | 34/356 [00:10<01:36,  3.33ba/s][ARunning tokenizer on train dataset #0:  10%|▉         | 34/356 [00:10<01:35,  3.36ba/s]


Running tokenizer on train dataset #3:  10%|▉         | 34/356 [00:10<01:36,  3.35ba/s][A[A[A

Running tokenizer on train dataset #2:  10%|▉         | 35/356 [00:10<01:36,  3.34ba/s][A[A
Running tokenizer on train dataset #1:  10%|▉         | 35/356 [00:10<01:36,  3.34ba/s][ARunning tokenizer on train dataset #0:  10%|▉         | 35/356 [00:10<01:35,  3.37ba/s]


Running tokenizer on train dataset #3:  10%|▉         | 35/356 [00:10<01:35,  3.36ba/s][A[A[A

Running tokenizer on train dataset #2:  10%|█         | 36/356 [00:11<01:35,  3.35ba/s][A[A
Running tokenizer on train dataset #1:  10%|█         | 36/356 [00:11<01:36,  3.32ba/s][ARunning tokenizer on train dataset #0:  10%|█         | 36/356 [00:11<01:35,  3.36ba/s]


Running tokenizer on train dataset #3:  10%|█         | 36/356 [00:11<01:35,  3.35ba/s][A[A[A

Running tokenizer on train dataset #2:  10%|█         | 37/356 [00:11<01:35,  3.34ba/s][A[A
Running tokenizer on train dataset #1:  10%|█         | 37/356 [00:11<01:35,  3.36ba/s][ARunning tokenizer on train dataset #0:  10%|█         | 37/356 [00:11<01:34,  3.38ba/s]


Running tokenizer on train dataset #3:  10%|█         | 37/356 [00:11<01:34,  3.36ba/s][A[A[A

Running tokenizer on train dataset #2:  11%|█         | 38/356 [00:11<01:35,  3.35ba/s][A[A
Running tokenizer on train dataset #1:  11%|█         | 38/356 [00:11<01:34,  3.37ba/s][ARunning tokenizer on train dataset #0:  11%|█         | 38/356 [00:11<01:33,  3.40ba/s]


Running tokenizer on train dataset #3:  11%|█         | 38/356 [00:11<01:34,  3.36ba/s][A[A[A

Running tokenizer on train dataset #2:  11%|█         | 39/356 [00:12<01:40,  3.15ba/s][A[A
Running tokenizer on train dataset #1:  11%|█         | 39/356 [00:12<01:39,  3.19ba/s][ARunning tokenizer on train dataset #0:  11%|█         | 39/356 [00:12<01:42,  3.10ba/s]


Running tokenizer on train dataset #3:  11%|█         | 39/356 [00:12<01:39,  3.18ba/s][A[A[A

Running tokenizer on train dataset #2:  11%|█         | 40/356 [00:12<01:38,  3.21ba/s][A[A
Running tokenizer on train dataset #1:  11%|█         | 40/356 [00:12<01:37,  3.25ba/s][ARunning tokenizer on train dataset #0:  11%|█         | 40/356 [00:12<01:39,  3.17ba/s]


Running tokenizer on train dataset #3:  11%|█         | 40/356 [00:12<01:37,  3.24ba/s][A[A[A

Running tokenizer on train dataset #2:  12%|█▏        | 41/356 [00:12<01:37,  3.24ba/s][A[A
Running tokenizer on train dataset #1:  12%|█▏        | 41/356 [00:12<01:35,  3.31ba/s][ARunning tokenizer on train dataset #0:  12%|█▏        | 41/356 [00:12<01:37,  3.24ba/s]


Running tokenizer on train dataset #3:  12%|█▏        | 41/356 [00:12<01:36,  3.28ba/s][A[A[A

Running tokenizer on train dataset #2:  12%|█▏        | 42/356 [00:12<01:35,  3.30ba/s][A[A
Running tokenizer on train dataset #1:  12%|█▏        | 42/356 [00:12<01:35,  3.27ba/s][ARunning tokenizer on train dataset #0:  12%|█▏        | 42/356 [00:13<01:35,  3.28ba/s]


Running tokenizer on train dataset #3:  12%|█▏        | 42/356 [00:13<01:34,  3.32ba/s][A[A[A

Running tokenizer on train dataset #2:  12%|█▏        | 43/356 [00:13<01:34,  3.31ba/s][A[A
Running tokenizer on train dataset #1:  12%|█▏        | 43/356 [00:13<01:34,  3.31ba/s][ARunning tokenizer on train dataset #0:  12%|█▏        | 43/356 [00:13<01:34,  3.33ba/s]


Running tokenizer on train dataset #3:  12%|█▏        | 43/356 [00:13<01:33,  3.36ba/s][A[A[A

Running tokenizer on train dataset #2:  12%|█▏        | 44/356 [00:13<01:34,  3.30ba/s][A[A
Running tokenizer on train dataset #1:  12%|█▏        | 44/356 [00:13<01:33,  3.34ba/s][ARunning tokenizer on train dataset #0:  12%|█▏        | 44/356 [00:13<01:33,  3.35ba/s]


Running tokenizer on train dataset #3:  12%|█▏        | 44/356 [00:13<01:32,  3.37ba/s][A[A[A

Running tokenizer on train dataset #2:  13%|█▎        | 45/356 [00:13<01:33,  3.33ba/s][A[A
Running tokenizer on train dataset #1:  13%|█▎        | 45/356 [00:13<01:32,  3.35ba/s][ARunning tokenizer on train dataset #0:  13%|█▎        | 45/356 [00:13<01:32,  3.35ba/s]


Running tokenizer on train dataset #3:  13%|█▎        | 45/356 [00:13<01:32,  3.37ba/s][A[A[A

Running tokenizer on train dataset #2:  13%|█▎        | 46/356 [00:14<01:32,  3.35ba/s][A[A
Running tokenizer on train dataset #1:  13%|█▎        | 46/356 [00:14<01:31,  3.37ba/s][ARunning tokenizer on train dataset #0:  13%|█▎        | 46/356 [00:14<01:32,  3.36ba/s]


Running tokenizer on train dataset #3:  13%|█▎        | 46/356 [00:14<01:32,  3.37ba/s][A[A[A

Running tokenizer on train dataset #2:  13%|█▎        | 47/356 [00:14<01:32,  3.34ba/s][A[A
Running tokenizer on train dataset #1:  13%|█▎        | 47/356 [00:14<01:31,  3.37ba/s][ARunning tokenizer on train dataset #0:  13%|█▎        | 47/356 [00:14<01:31,  3.37ba/s]


Running tokenizer on train dataset #3:  13%|█▎        | 47/356 [00:14<01:31,  3.37ba/s][A[A[A
Running tokenizer on train dataset #1:  13%|█▎        | 48/356 [00:14<01:31,  3.37ba/s][A

Running tokenizer on train dataset #2:  13%|█▎        | 48/356 [00:14<01:36,  3.20ba/s][A[ARunning tokenizer on train dataset #0:  13%|█▎        | 48/356 [00:14<01:30,  3.39ba/s]


Running tokenizer on train dataset #3:  13%|█▎        | 48/356 [00:14<01:31,  3.37ba/s][A[A[A
Running tokenizer on train dataset #1:  14%|█▍        | 49/356 [00:15<01:30,  3.39ba/s][A

Running tokenizer on train dataset #2:  14%|█▍        | 49/356 [00:15<01:34,  3.25ba/s][A[ARunning tokenizer on train dataset #0:  14%|█▍        | 49/356 [00:15<01:31,  3.37ba/s]


Running tokenizer on train dataset #3:  14%|█▍        | 49/356 [00:15<01:30,  3.38ba/s][A[A[A
Running tokenizer on train dataset #1:  14%|█▍        | 50/356 [00:15<01:35,  3.19ba/s][A

Running tokenizer on train dataset #2:  14%|█▍        | 50/356 [00:15<01:39,  3.08ba/s][A[ARunning tokenizer on train dataset #0:  14%|█▍        | 50/356 [00:15<01:36,  3.18ba/s]


Running tokenizer on train dataset #3:  14%|█▍        | 50/356 [00:15<01:35,  3.19ba/s][A[A[A
Running tokenizer on train dataset #1:  14%|█▍        | 51/356 [00:15<01:34,  3.23ba/s][A

Running tokenizer on train dataset #2:  14%|█▍        | 51/356 [00:15<01:36,  3.14ba/s][A[ARunning tokenizer on train dataset #0:  14%|█▍        | 51/356 [00:15<01:33,  3.25ba/s]


Running tokenizer on train dataset #3:  14%|█▍        | 51/356 [00:15<01:33,  3.25ba/s][A[A[A
Running tokenizer on train dataset #1:  15%|█▍        | 52/356 [00:15<01:33,  3.27ba/s][A

Running tokenizer on train dataset #2:  15%|█▍        | 52/356 [00:16<01:35,  3.19ba/s][A[ARunning tokenizer on train dataset #0:  15%|█▍        | 52/356 [00:16<01:32,  3.29ba/s]


Running tokenizer on train dataset #3:  15%|█▍        | 52/356 [00:16<01:32,  3.29ba/s][A[A[A
Running tokenizer on train dataset #1:  15%|█▍        | 53/356 [00:16<01:31,  3.30ba/s][A

Running tokenizer on train dataset #2:  15%|█▍        | 53/356 [00:16<01:33,  3.24ba/s][A[ARunning tokenizer on train dataset #0:  15%|█▍        | 53/356 [00:16<01:31,  3.32ba/s]


Running tokenizer on train dataset #3:  15%|█▍        | 53/356 [00:16<01:34,  3.22ba/s][A[A[A
Running tokenizer on train dataset #1:  15%|█▌        | 54/356 [00:16<01:31,  3.31ba/s][A

Running tokenizer on train dataset #2:  15%|█▌        | 54/356 [00:16<01:32,  3.27ba/s][A[ARunning tokenizer on train dataset #0:  15%|█▌        | 54/356 [00:16<01:34,  3.21ba/s]


Running tokenizer on train dataset #3:  15%|█▌        | 54/356 [00:16<01:32,  3.27ba/s][A[A[A
Running tokenizer on train dataset #1:  15%|█▌        | 55/356 [00:16<01:30,  3.33ba/s][A

Running tokenizer on train dataset #2:  15%|█▌        | 55/356 [00:16<01:31,  3.29ba/s][A[ARunning tokenizer on train dataset #0:  15%|█▌        | 55/356 [00:16<01:32,  3.27ba/s]


Running tokenizer on train dataset #3:  15%|█▌        | 55/356 [00:16<01:31,  3.30ba/s][A[A[A
Running tokenizer on train dataset #1:  16%|█▌        | 56/356 [00:17<01:29,  3.36ba/s][A

Running tokenizer on train dataset #2:  16%|█▌        | 56/356 [00:17<01:30,  3.31ba/s][A[ARunning tokenizer on train dataset #0:  16%|█▌        | 56/356 [00:17<01:30,  3.30ba/s]


Running tokenizer on train dataset #3:  16%|█▌        | 56/356 [00:17<01:30,  3.33ba/s][A[A[A
Running tokenizer on train dataset #1:  16%|█▌        | 57/356 [00:17<01:28,  3.38ba/s][A

Running tokenizer on train dataset #2:  16%|█▌        | 57/356 [00:17<01:30,  3.32ba/s][A[ARunning tokenizer on train dataset #0:  16%|█▌        | 57/356 [00:17<01:29,  3.33ba/s]


Running tokenizer on train dataset #3:  16%|█▌        | 57/356 [00:17<01:29,  3.33ba/s][A[A[A
Running tokenizer on train dataset #1:  16%|█▋        | 58/356 [00:17<01:28,  3.37ba/s][A

Running tokenizer on train dataset #2:  16%|█▋        | 58/356 [00:17<01:29,  3.32ba/s][A[ARunning tokenizer on train dataset #0:  16%|█▋        | 58/356 [00:17<01:29,  3.34ba/s]


Running tokenizer on train dataset #3:  16%|█▋        | 58/356 [00:17<01:29,  3.34ba/s][A[A[A
Running tokenizer on train dataset #1:  17%|█▋        | 59/356 [00:18<01:27,  3.38ba/s][A

Running tokenizer on train dataset #2:  17%|█▋        | 59/356 [00:18<01:28,  3.34ba/s][A[ARunning tokenizer on train dataset #0:  17%|█▋        | 59/356 [00:18<01:28,  3.35ba/s]


Running tokenizer on train dataset #3:  17%|█▋        | 59/356 [00:18<01:28,  3.34ba/s][A[A[A
Running tokenizer on train dataset #1:  17%|█▋        | 60/356 [00:18<01:27,  3.40ba/s][A

Running tokenizer on train dataset #2:  17%|█▋        | 60/356 [00:18<01:28,  3.34ba/s][A[ARunning tokenizer on train dataset #0:  17%|█▋        | 60/356 [00:18<01:28,  3.35ba/s]


Running tokenizer on train dataset #3:  17%|█▋        | 60/356 [00:18<01:28,  3.33ba/s][A[A[A
Running tokenizer on train dataset #1:  17%|█▋        | 61/356 [00:18<01:32,  3.20ba/s][A

Running tokenizer on train dataset #2:  17%|█▋        | 61/356 [00:18<01:33,  3.14ba/s][A[ARunning tokenizer on train dataset #0:  17%|█▋        | 61/356 [00:18<01:32,  3.18ba/s]


Running tokenizer on train dataset #3:  17%|█▋        | 61/356 [00:18<01:33,  3.16ba/s][A[A[A
Running tokenizer on train dataset #1:  17%|█▋        | 62/356 [00:18<01:31,  3.23ba/s][A

Running tokenizer on train dataset #2:  17%|█▋        | 62/356 [00:19<01:31,  3.20ba/s][A[ARunning tokenizer on train dataset #0:  17%|█▋        | 62/356 [00:19<01:30,  3.23ba/s]


Running tokenizer on train dataset #3:  17%|█▋        | 62/356 [00:19<01:30,  3.23ba/s][A[A[A
Running tokenizer on train dataset #1:  18%|█▊        | 63/356 [00:19<01:29,  3.26ba/s][A

Running tokenizer on train dataset #2:  18%|█▊        | 63/356 [00:19<01:30,  3.25ba/s][A[ARunning tokenizer on train dataset #0:  18%|█▊        | 63/356 [00:19<01:29,  3.29ba/s]


Running tokenizer on train dataset #3:  18%|█▊        | 63/356 [00:19<01:29,  3.28ba/s][A[A[A
Running tokenizer on train dataset #1:  18%|█▊        | 64/356 [00:19<01:28,  3.31ba/s][A

Running tokenizer on train dataset #2:  18%|█▊        | 64/356 [00:19<01:29,  3.26ba/s][A[ARunning tokenizer on train dataset #0:  18%|█▊        | 64/356 [00:19<01:27,  3.34ba/s]


Running tokenizer on train dataset #3:  18%|█▊        | 64/356 [00:19<01:28,  3.31ba/s][A[A[A
Running tokenizer on train dataset #1:  18%|█▊        | 65/356 [00:19<01:27,  3.32ba/s][A

Running tokenizer on train dataset #2:  18%|█▊        | 65/356 [00:19<01:28,  3.29ba/s][A[ARunning tokenizer on train dataset #0:  18%|█▊        | 65/356 [00:19<01:27,  3.34ba/s]


Running tokenizer on train dataset #3:  18%|█▊        | 65/356 [00:19<01:27,  3.34ba/s][A[A[A
Running tokenizer on train dataset #1:  19%|█▊        | 66/356 [00:20<01:26,  3.35ba/s][A

Running tokenizer on train dataset #2:  19%|█▊        | 66/356 [00:20<01:27,  3.33ba/s][A[ARunning tokenizer on train dataset #0:  19%|█▊        | 66/356 [00:20<01:26,  3.34ba/s]


Running tokenizer on train dataset #3:  19%|█▊        | 66/356 [00:20<01:26,  3.35ba/s][A[A[A
Running tokenizer on train dataset #1:  19%|█▉        | 67/356 [00:20<01:25,  3.37ba/s][A

Running tokenizer on train dataset #2:  19%|█▉        | 67/356 [00:20<01:26,  3.33ba/s][A[ARunning tokenizer on train dataset #0:  19%|█▉        | 67/356 [00:20<01:26,  3.35ba/s]


Running tokenizer on train dataset #3:  19%|█▉        | 67/356 [00:20<01:25,  3.38ba/s][A[A[A
Running tokenizer on train dataset #1:  19%|█▉        | 68/356 [00:20<01:25,  3.38ba/s][A

Running tokenizer on train dataset #2:  19%|█▉        | 68/356 [00:20<01:27,  3.30ba/s][A[ARunning tokenizer on train dataset #0:  19%|█▉        | 68/356 [00:20<01:25,  3.37ba/s]


Running tokenizer on train dataset #3:  19%|█▉        | 68/356 [00:20<01:25,  3.36ba/s][A[A[A


Running tokenizer on train dataset #3:  19%|█▉        | 69/356 [00:21<01:32,  3.11ba/s][A[A[ARunning tokenizer on train dataset #0:  19%|█▉        | 69/356 [00:21<01:33,  3.06ba/s]

Running tokenizer on train dataset #2:  19%|█▉        | 69/356 [00:21<01:35,  3.02ba/s][A[A
Running tokenizer on train dataset #1:  19%|█▉        | 69/356 [00:21<01:44,  2.74ba/s][A


Running tokenizer on train dataset #3:  20%|█▉        | 70/356 [00:21<01:29,  3.18ba/s][A[A[A
Running tokenizer on train dataset #1:  20%|█▉        | 70/356 [00:21<01:38,  2.90ba/s][A

Running tokenizer on train dataset #2:  20%|█▉        | 70/356 [00:21<01:32,  3.10ba/s][A[ARunning tokenizer on train dataset #0:  20%|█▉        | 70/356 [00:21<01:31,  3.13ba/s]


Running tokenizer on train dataset #3:  20%|█▉        | 71/356 [00:21<01:27,  3.25ba/s][A[A[A
Running tokenizer on train dataset #1:  20%|█▉        | 71/356 [00:21<01:34,  3.02ba/s][ARunning tokenizer on train dataset #0:  20%|█▉        | 71/356 [00:21<01:29,  3.20ba/s]

Running tokenizer on train dataset #2:  20%|█▉        | 71/356 [00:21<01:30,  3.16ba/s][A[A


Running tokenizer on train dataset #3:  20%|██        | 72/356 [00:22<01:31,  3.11ba/s][A[A[A
Running tokenizer on train dataset #1:  20%|██        | 72/356 [00:22<01:36,  2.95ba/s][ARunning tokenizer on train dataset #0:  20%|██        | 72/356 [00:22<01:32,  3.07ba/s]

Running tokenizer on train dataset #2:  20%|██        | 72/356 [00:22<01:33,  3.04ba/s][A[A


Running tokenizer on train dataset #3:  21%|██        | 73/356 [00:22<01:28,  3.19ba/s][A[A[ARunning tokenizer on train dataset #0:  21%|██        | 73/356 [00:22<01:29,  3.17ba/s]
Running tokenizer on train dataset #1:  21%|██        | 73/356 [00:22<01:32,  3.06ba/s][A

Running tokenizer on train dataset #2:  21%|██        | 73/356 [00:22<01:30,  3.13ba/s][A[A


Running tokenizer on train dataset #3:  21%|██        | 74/356 [00:22<01:27,  3.24ba/s][A[A[A
Running tokenizer on train dataset #1:  21%|██        | 74/356 [00:22<01:29,  3.16ba/s][ARunning tokenizer on train dataset #0:  21%|██        | 74/356 [00:22<01:27,  3.22ba/s]

Running tokenizer on train dataset #2:  21%|██        | 74/356 [00:22<01:28,  3.19ba/s][A[A


Running tokenizer on train dataset #3:  21%|██        | 75/356 [00:23<01:25,  3.29ba/s][A[A[ARunning tokenizer on train dataset #0:  21%|██        | 75/356 [00:23<01:26,  3.26ba/s]
Running tokenizer on train dataset #1:  21%|██        | 75/356 [00:23<01:27,  3.21ba/s][A

Running tokenizer on train dataset #2:  21%|██        | 75/356 [00:23<01:26,  3.24ba/s][A[A


Running tokenizer on train dataset #3:  21%|██▏       | 76/356 [00:23<01:24,  3.31ba/s][A[A[ARunning tokenizer on train dataset #0:  21%|██▏       | 76/356 [00:23<01:25,  3.29ba/s]
Running tokenizer on train dataset #1:  21%|██▏       | 76/356 [00:23<01:26,  3.25ba/s][A

Running tokenizer on train dataset #2:  21%|██▏       | 76/356 [00:23<01:25,  3.26ba/s][A[A


Running tokenizer on train dataset #3:  22%|██▏       | 77/356 [00:23<01:23,  3.33ba/s][A[A[A
Running tokenizer on train dataset #1:  22%|██▏       | 77/356 [00:23<01:24,  3.30ba/s][ARunning tokenizer on train dataset #0:  22%|██▏       | 77/356 [00:23<01:24,  3.30ba/s]

Running tokenizer on train dataset #2:  22%|██▏       | 77/356 [00:23<01:25,  3.28ba/s][A[A


Running tokenizer on train dataset #3:  22%|██▏       | 78/356 [00:23<01:23,  3.34ba/s][A[A[ARunning tokenizer on train dataset #0:  22%|██▏       | 78/356 [00:24<01:23,  3.33ba/s]
Running tokenizer on train dataset #1:  22%|██▏       | 78/356 [00:24<01:24,  3.29ba/s][A

Running tokenizer on train dataset #2:  22%|██▏       | 78/356 [00:24<01:24,  3.29ba/s][A[A


Running tokenizer on train dataset #3:  22%|██▏       | 79/356 [00:24<01:22,  3.36ba/s][A[A[ARunning tokenizer on train dataset #0:  22%|██▏       | 79/356 [00:24<01:22,  3.35ba/s]
Running tokenizer on train dataset #1:  22%|██▏       | 79/356 [00:24<01:23,  3.33ba/s][A

Running tokenizer on train dataset #2:  22%|██▏       | 79/356 [00:24<01:22,  3.34ba/s][A[A


Running tokenizer on train dataset #3:  22%|██▏       | 80/356 [00:24<01:22,  3.36ba/s][A[A[A
Running tokenizer on train dataset #1:  22%|██▏       | 80/356 [00:24<01:22,  3.34ba/s][ARunning tokenizer on train dataset #0:  22%|██▏       | 80/356 [00:24<01:22,  3.34ba/s]

Running tokenizer on train dataset #2:  22%|██▏       | 80/356 [00:24<01:23,  3.32ba/s][A[A


Running tokenizer on train dataset #3:  23%|██▎       | 81/356 [00:24<01:21,  3.36ba/s][A[A[A
Running tokenizer on train dataset #1:  23%|██▎       | 81/356 [00:24<01:22,  3.34ba/s][ARunning tokenizer on train dataset #0:  23%|██▎       | 81/356 [00:24<01:22,  3.34ba/s]

Running tokenizer on train dataset #2:  23%|██▎       | 81/356 [00:24<01:22,  3.34ba/s][A[A


Running tokenizer on train dataset #3:  23%|██▎       | 82/356 [00:25<01:21,  3.38ba/s][A[A[ARunning tokenizer on train dataset #0:  23%|██▎       | 82/356 [00:25<01:21,  3.35ba/s]
Running tokenizer on train dataset #1:  23%|██▎       | 82/356 [00:25<01:21,  3.34ba/s][A

Running tokenizer on train dataset #2:  23%|██▎       | 82/356 [00:25<01:21,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  23%|██▎       | 83/356 [00:25<01:26,  3.16ba/s][A[A[ARunning tokenizer on train dataset #0:  23%|██▎       | 83/356 [00:25<01:26,  3.15ba/s]
Running tokenizer on train dataset #1:  23%|██▎       | 83/356 [00:25<01:26,  3.14ba/s][A

Running tokenizer on train dataset #2:  23%|██▎       | 83/356 [00:25<01:26,  3.15ba/s][A[A


Running tokenizer on train dataset #3:  24%|██▎       | 84/356 [00:25<01:24,  3.23ba/s][A[A[ARunning tokenizer on train dataset #0:  24%|██▎       | 84/356 [00:25<01:24,  3.22ba/s]
Running tokenizer on train dataset #1:  24%|██▎       | 84/356 [00:25<01:24,  3.21ba/s][A

Running tokenizer on train dataset #2:  24%|██▎       | 84/356 [00:25<01:24,  3.22ba/s][A[A


Running tokenizer on train dataset #3:  24%|██▍       | 85/356 [00:26<01:22,  3.28ba/s][A[A[ARunning tokenizer on train dataset #0:  24%|██▍       | 85/356 [00:26<01:23,  3.26ba/s]
Running tokenizer on train dataset #1:  24%|██▍       | 85/356 [00:26<01:23,  3.24ba/s][A

Running tokenizer on train dataset #2:  24%|██▍       | 85/356 [00:26<01:23,  3.25ba/s][A[A


Running tokenizer on train dataset #3:  24%|██▍       | 86/356 [00:26<01:21,  3.31ba/s][A[A[ARunning tokenizer on train dataset #0:  24%|██▍       | 86/356 [00:26<01:22,  3.29ba/s]
Running tokenizer on train dataset #1:  24%|██▍       | 86/356 [00:26<01:22,  3.29ba/s][A

Running tokenizer on train dataset #2:  24%|██▍       | 86/356 [00:26<01:22,  3.26ba/s][A[A


Running tokenizer on train dataset #3:  24%|██▍       | 87/356 [00:26<01:21,  3.32ba/s][A[A[ARunning tokenizer on train dataset #0:  24%|██▍       | 87/356 [00:26<01:21,  3.31ba/s]
Running tokenizer on train dataset #1:  24%|██▍       | 87/356 [00:26<01:21,  3.31ba/s][A

Running tokenizer on train dataset #2:  24%|██▍       | 87/356 [00:26<01:21,  3.28ba/s][A[A


Running tokenizer on train dataset #3:  25%|██▍       | 88/356 [00:26<01:20,  3.34ba/s][A[A[A
Running tokenizer on train dataset #1:  25%|██▍       | 88/356 [00:27<01:20,  3.32ba/s][ARunning tokenizer on train dataset #0:  25%|██▍       | 88/356 [00:27<01:20,  3.31ba/s]

Running tokenizer on train dataset #2:  25%|██▍       | 88/356 [00:27<01:20,  3.32ba/s][A[A


Running tokenizer on train dataset #3:  25%|██▌       | 89/356 [00:27<01:19,  3.36ba/s][A[A[A
Running tokenizer on train dataset #1:  25%|██▌       | 89/356 [00:27<01:20,  3.32ba/s][ARunning tokenizer on train dataset #0:  25%|██▌       | 89/356 [00:27<01:20,  3.32ba/s]

Running tokenizer on train dataset #2:  25%|██▌       | 89/356 [00:27<01:20,  3.33ba/s][A[A


Running tokenizer on train dataset #3:  25%|██▌       | 90/356 [00:27<01:19,  3.36ba/s][A[A[A
Running tokenizer on train dataset #1:  25%|██▌       | 90/356 [00:27<01:19,  3.33ba/s][ARunning tokenizer on train dataset #0:  25%|██▌       | 90/356 [00:27<01:20,  3.32ba/s]

Running tokenizer on train dataset #2:  25%|██▌       | 90/356 [00:27<01:19,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  26%|██▌       | 91/356 [00:27<01:19,  3.35ba/s][A[A[ARunning tokenizer on train dataset #0:  26%|██▌       | 91/356 [00:27<01:19,  3.34ba/s]
Running tokenizer on train dataset #1:  26%|██▌       | 91/356 [00:27<01:19,  3.34ba/s][A

Running tokenizer on train dataset #2:  26%|██▌       | 91/356 [00:27<01:18,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  26%|██▌       | 92/356 [00:28<01:18,  3.37ba/s][A[A[A
Running tokenizer on train dataset #1:  26%|██▌       | 92/356 [00:28<01:18,  3.35ba/s][ARunning tokenizer on train dataset #0:  26%|██▌       | 92/356 [00:28<01:18,  3.35ba/s]

Running tokenizer on train dataset #2:  26%|██▌       | 92/356 [00:28<01:18,  3.37ba/s][A[A


Running tokenizer on train dataset #3:  26%|██▌       | 93/356 [00:28<01:18,  3.37ba/s][A[A[A
Running tokenizer on train dataset #1:  26%|██▌       | 93/356 [00:28<01:18,  3.36ba/s][ARunning tokenizer on train dataset #0:  26%|██▌       | 93/356 [00:28<01:18,  3.34ba/s]

Running tokenizer on train dataset #2:  26%|██▌       | 93/356 [00:28<01:18,  3.37ba/s][A[A


Running tokenizer on train dataset #3:  26%|██▋       | 94/356 [00:28<01:22,  3.19ba/s][A[A[A
Running tokenizer on train dataset #1:  26%|██▋       | 94/356 [00:28<01:22,  3.17ba/s][ARunning tokenizer on train dataset #0:  26%|██▋       | 94/356 [00:28<01:22,  3.16ba/s]

Running tokenizer on train dataset #2:  26%|██▋       | 94/356 [00:28<01:23,  3.16ba/s][A[A


Running tokenizer on train dataset #3:  27%|██▋       | 95/356 [00:29<01:20,  3.25ba/s][A[A[A
Running tokenizer on train dataset #1:  27%|██▋       | 95/356 [00:29<01:20,  3.23ba/s][ARunning tokenizer on train dataset #0:  27%|██▋       | 95/356 [00:29<01:20,  3.23ba/s]

Running tokenizer on train dataset #2:  27%|██▋       | 95/356 [00:29<01:21,  3.22ba/s][A[A


Running tokenizer on train dataset #3:  27%|██▋       | 96/356 [00:29<01:18,  3.30ba/s][A[A[ARunning tokenizer on train dataset #0:  27%|██▋       | 96/356 [00:29<01:19,  3.28ba/s]
Running tokenizer on train dataset #1:  27%|██▋       | 96/356 [00:29<01:19,  3.26ba/s][A

Running tokenizer on train dataset #2:  27%|██▋       | 96/356 [00:29<01:20,  3.25ba/s][A[A


Running tokenizer on train dataset #3:  27%|██▋       | 97/356 [00:29<01:18,  3.31ba/s][A[A[ARunning tokenizer on train dataset #0:  27%|██▋       | 97/356 [00:29<01:18,  3.32ba/s]
Running tokenizer on train dataset #1:  27%|██▋       | 97/356 [00:29<01:18,  3.29ba/s][A

Running tokenizer on train dataset #2:  27%|██▋       | 97/356 [00:29<01:18,  3.29ba/s][A[A


Running tokenizer on train dataset #3:  28%|██▊       | 98/356 [00:30<01:17,  3.33ba/s][A[A[ARunning tokenizer on train dataset #0:  28%|██▊       | 98/356 [00:30<01:16,  3.35ba/s]
Running tokenizer on train dataset #1:  28%|██▊       | 98/356 [00:30<01:17,  3.33ba/s][A

Running tokenizer on train dataset #2:  28%|██▊       | 98/356 [00:30<01:18,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  28%|██▊       | 99/356 [00:30<01:17,  3.34ba/s][A[A[ARunning tokenizer on train dataset #0:  28%|██▊       | 99/356 [00:30<01:16,  3.36ba/s]
Running tokenizer on train dataset #1:  28%|██▊       | 99/356 [00:30<01:17,  3.32ba/s][A

Running tokenizer on train dataset #2:  28%|██▊       | 99/356 [00:30<01:17,  3.32ba/s][A[A


Running tokenizer on train dataset #3:  28%|██▊       | 100/356 [00:30<01:16,  3.36ba/s][A[A[ARunning tokenizer on train dataset #0:  28%|██▊       | 100/356 [00:30<01:16,  3.36ba/s]
Running tokenizer on train dataset #1:  28%|██▊       | 100/356 [00:30<01:16,  3.33ba/s][A

Running tokenizer on train dataset #2:  28%|██▊       | 100/356 [00:30<01:17,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  28%|██▊       | 101/356 [00:30<01:15,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  28%|██▊       | 101/356 [00:30<01:15,  3.37ba/s]
Running tokenizer on train dataset #1:  28%|██▊       | 101/356 [00:30<01:16,  3.35ba/s][A

Running tokenizer on train dataset #2:  28%|██▊       | 101/356 [00:30<01:16,  3.32ba/s][A[A


Running tokenizer on train dataset #3:  29%|██▊       | 102/356 [00:31<01:15,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  29%|██▊       | 102/356 [00:31<01:15,  3.37ba/s]
Running tokenizer on train dataset #1:  29%|██▊       | 102/356 [00:31<01:15,  3.36ba/s][A

Running tokenizer on train dataset #2:  29%|██▊       | 102/356 [00:31<01:16,  3.33ba/s][A[A


Running tokenizer on train dataset #3:  29%|██▉       | 103/356 [00:31<01:15,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  29%|██▉       | 103/356 [00:31<01:15,  3.36ba/s]
Running tokenizer on train dataset #1:  29%|██▉       | 103/356 [00:31<01:15,  3.37ba/s][A

Running tokenizer on train dataset #2:  29%|██▉       | 103/356 [00:31<01:15,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  29%|██▉       | 104/356 [00:31<01:14,  3.39ba/s][A[A[ARunning tokenizer on train dataset #0:  29%|██▉       | 104/356 [00:31<01:15,  3.35ba/s]
Running tokenizer on train dataset #1:  29%|██▉       | 104/356 [00:31<01:14,  3.36ba/s][A

Running tokenizer on train dataset #2:  29%|██▉       | 104/356 [00:31<01:15,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  29%|██▉       | 105/356 [00:32<01:18,  3.18ba/s][A[A[ARunning tokenizer on train dataset #0:  29%|██▉       | 105/356 [00:32<01:19,  3.15ba/s]
Running tokenizer on train dataset #1:  29%|██▉       | 105/356 [00:32<01:19,  3.16ba/s][A

Running tokenizer on train dataset #2:  29%|██▉       | 105/356 [00:32<01:19,  3.16ba/s][A[A


Running tokenizer on train dataset #3:  30%|██▉       | 106/356 [00:32<01:16,  3.26ba/s][A[A[ARunning tokenizer on train dataset #0:  30%|██▉       | 106/356 [00:32<01:17,  3.21ba/s]
Running tokenizer on train dataset #1:  30%|██▉       | 106/356 [00:32<01:17,  3.22ba/s][A

Running tokenizer on train dataset #2:  30%|██▉       | 106/356 [00:32<01:17,  3.23ba/s][A[A


Running tokenizer on train dataset #3:  30%|███       | 107/356 [00:32<01:15,  3.28ba/s][A[A[ARunning tokenizer on train dataset #0:  30%|███       | 107/356 [00:32<01:16,  3.25ba/s]
Running tokenizer on train dataset #1:  30%|███       | 107/356 [00:32<01:16,  3.26ba/s][A

Running tokenizer on train dataset #2:  30%|███       | 107/356 [00:32<01:16,  3.26ba/s][A[A


Running tokenizer on train dataset #3:  30%|███       | 108/356 [00:33<01:14,  3.31ba/s][A[A[ARunning tokenizer on train dataset #0:  30%|███       | 108/356 [00:33<01:15,  3.29ba/s]
Running tokenizer on train dataset #1:  30%|███       | 108/356 [00:33<01:15,  3.29ba/s][A

Running tokenizer on train dataset #2:  30%|███       | 108/356 [00:33<01:14,  3.31ba/s][A[A


Running tokenizer on train dataset #3:  31%|███       | 109/356 [00:33<01:14,  3.33ba/s][A[A[ARunning tokenizer on train dataset #0:  31%|███       | 109/356 [00:33<01:14,  3.31ba/s]
Running tokenizer on train dataset #1:  31%|███       | 109/356 [00:33<01:14,  3.33ba/s][A

Running tokenizer on train dataset #2:  31%|███       | 109/356 [00:33<01:14,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  31%|███       | 110/356 [00:33<01:13,  3.36ba/s][A[A[ARunning tokenizer on train dataset #0:  31%|███       | 110/356 [00:33<01:13,  3.33ba/s]
Running tokenizer on train dataset #1:  31%|███       | 110/356 [00:33<01:14,  3.32ba/s][A

Running tokenizer on train dataset #2:  31%|███       | 110/356 [00:33<01:13,  3.34ba/s][A[A


Running tokenizer on train dataset #3:  31%|███       | 111/356 [00:33<01:13,  3.35ba/s][A[A[ARunning tokenizer on train dataset #0:  31%|███       | 111/356 [00:34<01:13,  3.35ba/s]
Running tokenizer on train dataset #1:  31%|███       | 111/356 [00:34<01:13,  3.34ba/s][A

Running tokenizer on train dataset #2:  31%|███       | 111/356 [00:34<01:13,  3.32ba/s][A[A


Running tokenizer on train dataset #3:  31%|███▏      | 112/356 [00:34<01:12,  3.36ba/s][A[A[ARunning tokenizer on train dataset #0:  31%|███▏      | 112/356 [00:34<01:12,  3.35ba/s]
Running tokenizer on train dataset #1:  31%|███▏      | 112/356 [00:34<01:12,  3.35ba/s][A

Running tokenizer on train dataset #2:  31%|███▏      | 112/356 [00:34<01:13,  3.33ba/s][A[A


Running tokenizer on train dataset #3:  32%|███▏      | 113/356 [00:34<01:11,  3.38ba/s][A[A[ARunning tokenizer on train dataset #0:  32%|███▏      | 113/356 [00:34<01:12,  3.35ba/s]
Running tokenizer on train dataset #1:  32%|███▏      | 113/356 [00:34<01:12,  3.36ba/s][A

Running tokenizer on train dataset #2:  32%|███▏      | 113/356 [00:34<01:12,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  32%|███▏      | 114/356 [00:34<01:11,  3.36ba/s][A[A[ARunning tokenizer on train dataset #0:  32%|███▏      | 114/356 [00:34<01:12,  3.36ba/s]
Running tokenizer on train dataset #1:  32%|███▏      | 114/356 [00:34<01:12,  3.36ba/s][A

Running tokenizer on train dataset #2:  32%|███▏      | 114/356 [00:34<01:11,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  32%|███▏      | 115/356 [00:35<01:11,  3.35ba/s][A[A[ARunning tokenizer on train dataset #0:  32%|███▏      | 115/356 [00:35<01:11,  3.37ba/s]
Running tokenizer on train dataset #1:  32%|███▏      | 115/356 [00:35<01:11,  3.37ba/s][A

Running tokenizer on train dataset #2:  32%|███▏      | 115/356 [00:35<01:11,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  33%|███▎      | 116/356 [00:35<01:15,  3.16ba/s][A[A[ARunning tokenizer on train dataset #0:  33%|███▎      | 116/356 [00:35<01:15,  3.17ba/s]
Running tokenizer on train dataset #1:  33%|███▎      | 116/356 [00:35<01:15,  3.19ba/s][A

Running tokenizer on train dataset #2:  33%|███▎      | 116/356 [00:35<01:16,  3.15ba/s][A[A


Running tokenizer on train dataset #3:  33%|███▎      | 117/356 [00:35<01:14,  3.22ba/s][A[A[ARunning tokenizer on train dataset #0:  33%|███▎      | 117/356 [00:35<01:14,  3.23ba/s]
Running tokenizer on train dataset #1:  33%|███▎      | 117/356 [00:35<01:13,  3.24ba/s][A

Running tokenizer on train dataset #2:  33%|███▎      | 117/356 [00:35<01:14,  3.22ba/s][A[A


Running tokenizer on train dataset #3:  33%|███▎      | 118/356 [00:36<01:12,  3.29ba/s][A[A[ARunning tokenizer on train dataset #0:  33%|███▎      | 118/356 [00:36<01:12,  3.27ba/s]
Running tokenizer on train dataset #1:  33%|███▎      | 118/356 [00:36<01:12,  3.29ba/s][A

Running tokenizer on train dataset #2:  33%|███▎      | 118/356 [00:36<01:13,  3.26ba/s][A[A


Running tokenizer on train dataset #3:  33%|███▎      | 119/356 [00:36<01:11,  3.32ba/s][A[A[A
Running tokenizer on train dataset #1:  33%|███▎      | 119/356 [00:36<01:11,  3.30ba/s][ARunning tokenizer on train dataset #0:  33%|███▎      | 119/356 [00:36<01:12,  3.29ba/s]

Running tokenizer on train dataset #2:  33%|███▎      | 119/356 [00:36<01:11,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  34%|███▎      | 120/356 [00:36<01:10,  3.33ba/s][A[A[A
Running tokenizer on train dataset #1:  34%|███▎      | 120/356 [00:36<01:10,  3.33ba/s][ARunning tokenizer on train dataset #0:  34%|███▎      | 120/356 [00:36<01:11,  3.32ba/s]

Running tokenizer on train dataset #2:  34%|███▎      | 120/356 [00:36<01:11,  3.31ba/s][A[A


Running tokenizer on train dataset #3:  34%|███▍      | 121/356 [00:36<01:10,  3.35ba/s][A[A[A
Running tokenizer on train dataset #1:  34%|███▍      | 121/356 [00:37<01:10,  3.33ba/s][ARunning tokenizer on train dataset #0:  34%|███▍      | 121/356 [00:37<01:10,  3.33ba/s]

Running tokenizer on train dataset #2:  34%|███▍      | 121/356 [00:37<01:10,  3.33ba/s][A[A


Running tokenizer on train dataset #3:  34%|███▍      | 122/356 [00:37<01:09,  3.35ba/s][A[A[A
Running tokenizer on train dataset #1:  34%|███▍      | 122/356 [00:37<01:09,  3.35ba/s][ARunning tokenizer on train dataset #0:  34%|███▍      | 122/356 [00:37<01:10,  3.33ba/s]

Running tokenizer on train dataset #2:  34%|███▍      | 122/356 [00:37<01:09,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  35%|███▍      | 123/356 [00:37<01:09,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  35%|███▍      | 123/356 [00:37<01:10,  3.33ba/s]

Running tokenizer on train dataset #2:  35%|███▍      | 123/356 [00:37<01:09,  3.36ba/s][A[A
Running tokenizer on train dataset #1:  35%|███▍      | 123/356 [00:37<01:14,  3.14ba/s][A


Running tokenizer on train dataset #3:  35%|███▍      | 124/356 [00:37<01:08,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  35%|███▍      | 124/356 [00:37<01:09,  3.36ba/s]

Running tokenizer on train dataset #2:  35%|███▍      | 124/356 [00:37<01:08,  3.36ba/s][A[A
Running tokenizer on train dataset #1:  35%|███▍      | 124/356 [00:37<01:12,  3.20ba/s][A


Running tokenizer on train dataset #3:  35%|███▌      | 125/356 [00:38<01:08,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  35%|███▌      | 125/356 [00:38<01:08,  3.36ba/s]

Running tokenizer on train dataset #2:  35%|███▌      | 125/356 [00:38<01:08,  3.36ba/s][A[A
Running tokenizer on train dataset #1:  35%|███▌      | 125/356 [00:38<01:11,  3.25ba/s][A


Running tokenizer on train dataset #3:  35%|███▌      | 126/356 [00:38<01:08,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  35%|███▌      | 126/356 [00:38<01:08,  3.36ba/s]

Running tokenizer on train dataset #2:  35%|███▌      | 126/356 [00:38<01:08,  3.37ba/s][A[A
Running tokenizer on train dataset #1:  35%|███▌      | 126/356 [00:38<01:09,  3.30ba/s][A


Running tokenizer on train dataset #3:  36%|███▌      | 127/356 [00:38<01:12,  3.18ba/s][A[A[ARunning tokenizer on train dataset #0:  36%|███▌      | 127/356 [00:38<01:12,  3.15ba/s]

Running tokenizer on train dataset #2:  36%|███▌      | 127/356 [00:38<01:12,  3.17ba/s][A[A
Running tokenizer on train dataset #1:  36%|███▌      | 127/356 [00:38<01:12,  3.15ba/s][A


Running tokenizer on train dataset #3:  36%|███▌      | 128/356 [00:39<01:10,  3.26ba/s][A[A[ARunning tokenizer on train dataset #0:  36%|███▌      | 128/356 [00:39<01:10,  3.22ba/s]

Running tokenizer on train dataset #2:  36%|███▌      | 128/356 [00:39<01:10,  3.21ba/s][A[A
Running tokenizer on train dataset #1:  36%|███▌      | 128/356 [00:39<01:10,  3.22ba/s][A


Running tokenizer on train dataset #3:  36%|███▌      | 129/356 [00:39<01:29,  2.54ba/s][A[A[A
Running tokenizer on train dataset #1:  36%|███▌      | 129/356 [00:39<01:20,  2.83ba/s][ARunning tokenizer on train dataset #0:  36%|███▌      | 129/356 [00:39<01:23,  2.73ba/s]

Running tokenizer on train dataset #2:  36%|███▌      | 129/356 [00:39<01:20,  2.80ba/s][A[ARunning tokenizer on train dataset #0:  37%|███▋      | 130/356 [00:39<01:17,  2.90ba/s]


Running tokenizer on train dataset #3:  37%|███▋      | 130/356 [00:39<01:22,  2.74ba/s][A[A[A
Running tokenizer on train dataset #1:  37%|███▋      | 130/356 [00:39<01:16,  2.97ba/s][A

Running tokenizer on train dataset #2:  37%|███▋      | 130/356 [00:39<01:16,  2.95ba/s][A[A
Running tokenizer on train dataset #1:  37%|███▋      | 131/356 [00:40<01:12,  3.08ba/s][A


Running tokenizer on train dataset #3:  37%|███▋      | 131/356 [00:40<01:17,  2.89ba/s][A[A[ARunning tokenizer on train dataset #0:  37%|███▋      | 131/356 [00:40<01:14,  3.01ba/s]

Running tokenizer on train dataset #2:  37%|███▋      | 131/356 [00:40<01:13,  3.04ba/s][A[ARunning tokenizer on train dataset #0:  37%|███▋      | 132/356 [00:40<01:11,  3.12ba/s]
Running tokenizer on train dataset #1:  37%|███▋      | 132/356 [00:40<01:10,  3.17ba/s][A


Running tokenizer on train dataset #3:  37%|███▋      | 132/356 [00:40<01:14,  3.02ba/s][A[A[A

Running tokenizer on train dataset #2:  37%|███▋      | 132/356 [00:40<01:11,  3.12ba/s][A[A
Running tokenizer on train dataset #1:  37%|███▋      | 133/356 [00:40<01:09,  3.22ba/s][ARunning tokenizer on train dataset #0:  37%|███▋      | 133/356 [00:40<01:10,  3.17ba/s]


Running tokenizer on train dataset #3:  37%|███▋      | 133/356 [00:40<01:11,  3.11ba/s][A[A[A

Running tokenizer on train dataset #2:  37%|███▋      | 133/356 [00:40<01:09,  3.20ba/s][A[A
Running tokenizer on train dataset #1:  38%|███▊      | 134/356 [00:41<01:08,  3.26ba/s][A


Running tokenizer on train dataset #3:  38%|███▊      | 134/356 [00:41<01:09,  3.18ba/s][A[A[ARunning tokenizer on train dataset #0:  38%|███▊      | 134/356 [00:41<01:08,  3.22ba/s]

Running tokenizer on train dataset #2:  38%|███▊      | 134/356 [00:41<01:08,  3.24ba/s][A[A


Running tokenizer on train dataset #3:  38%|███▊      | 135/356 [00:41<01:08,  3.24ba/s][A[A[ARunning tokenizer on train dataset #0:  38%|███▊      | 135/356 [00:41<01:07,  3.27ba/s]
Running tokenizer on train dataset #1:  38%|███▊      | 135/356 [00:41<01:07,  3.28ba/s][A

Running tokenizer on train dataset #2:  38%|███▊      | 135/356 [00:41<01:07,  3.28ba/s][A[A


Running tokenizer on train dataset #3:  38%|███▊      | 136/356 [00:41<01:07,  3.28ba/s][A[A[A
Running tokenizer on train dataset #1:  38%|███▊      | 136/356 [00:41<01:06,  3.30ba/s][ARunning tokenizer on train dataset #0:  38%|███▊      | 136/356 [00:41<01:06,  3.28ba/s]

Running tokenizer on train dataset #2:  38%|███▊      | 136/356 [00:41<01:07,  3.27ba/s][A[A


Running tokenizer on train dataset #3:  38%|███▊      | 137/356 [00:42<01:06,  3.32ba/s][A[A[ARunning tokenizer on train dataset #0:  38%|███▊      | 137/356 [00:42<01:05,  3.32ba/s]
Running tokenizer on train dataset #1:  38%|███▊      | 137/356 [00:42<01:05,  3.32ba/s][A

Running tokenizer on train dataset #2:  38%|███▊      | 137/356 [00:42<01:06,  3.31ba/s][A[A


Running tokenizer on train dataset #3:  39%|███▉      | 138/356 [00:42<01:09,  3.14ba/s][A[A[A
Running tokenizer on train dataset #1:  39%|███▉      | 138/356 [00:42<01:09,  3.14ba/s][ARunning tokenizer on train dataset #0:  39%|███▉      | 138/356 [00:42<01:09,  3.13ba/s]

Running tokenizer on train dataset #2:  39%|███▉      | 138/356 [00:42<01:09,  3.11ba/s][A[A


Running tokenizer on train dataset #3:  39%|███▉      | 139/356 [00:42<01:07,  3.20ba/s][A[A[A
Running tokenizer on train dataset #1:  39%|███▉      | 139/356 [00:42<01:07,  3.20ba/s][ARunning tokenizer on train dataset #0:  39%|███▉      | 139/356 [00:42<01:08,  3.18ba/s]

Running tokenizer on train dataset #2:  39%|███▉      | 139/356 [00:42<01:08,  3.19ba/s][A[A


Running tokenizer on train dataset #3:  39%|███▉      | 140/356 [00:42<01:06,  3.25ba/s][A[A[A
Running tokenizer on train dataset #1:  39%|███▉      | 140/356 [00:43<01:06,  3.26ba/s][ARunning tokenizer on train dataset #0:  39%|███▉      | 140/356 [00:43<01:06,  3.25ba/s]

Running tokenizer on train dataset #2:  39%|███▉      | 140/356 [00:43<01:06,  3.24ba/s][A[A


Running tokenizer on train dataset #3:  40%|███▉      | 141/356 [00:43<01:05,  3.30ba/s][A[A[A
Running tokenizer on train dataset #1:  40%|███▉      | 141/356 [00:43<01:05,  3.29ba/s][ARunning tokenizer on train dataset #0:  40%|███▉      | 141/356 [00:43<01:05,  3.26ba/s]

Running tokenizer on train dataset #2:  40%|███▉      | 141/356 [00:43<01:05,  3.27ba/s][A[A


Running tokenizer on train dataset #3:  40%|███▉      | 142/356 [00:43<01:04,  3.33ba/s][A[A[A
Running tokenizer on train dataset #1:  40%|███▉      | 142/356 [00:43<01:04,  3.31ba/s][ARunning tokenizer on train dataset #0:  40%|███▉      | 142/356 [00:43<01:04,  3.30ba/s]

Running tokenizer on train dataset #2:  40%|███▉      | 142/356 [00:43<01:04,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  40%|████      | 143/356 [00:43<01:03,  3.35ba/s][A[A[A
Running tokenizer on train dataset #1:  40%|████      | 143/356 [00:43<01:03,  3.34ba/s][ARunning tokenizer on train dataset #0:  40%|████      | 143/356 [00:43<01:04,  3.31ba/s]

Running tokenizer on train dataset #2:  40%|████      | 143/356 [00:43<01:04,  3.31ba/s][A[A


Running tokenizer on train dataset #3:  40%|████      | 144/356 [00:44<01:02,  3.37ba/s][A[A[A
Running tokenizer on train dataset #1:  40%|████      | 144/356 [00:44<01:02,  3.37ba/s][ARunning tokenizer on train dataset #0:  40%|████      | 144/356 [00:44<01:03,  3.32ba/s]

Running tokenizer on train dataset #2:  40%|████      | 144/356 [00:44<01:03,  3.32ba/s][A[A


Running tokenizer on train dataset #3:  41%|████      | 145/356 [00:44<01:02,  3.38ba/s][A[A[A
Running tokenizer on train dataset #1:  41%|████      | 145/356 [00:44<01:02,  3.36ba/s][ARunning tokenizer on train dataset #0:  41%|████      | 145/356 [00:44<01:03,  3.32ba/s]

Running tokenizer on train dataset #2:  41%|████      | 145/356 [00:44<01:03,  3.33ba/s][A[A


Running tokenizer on train dataset #3:  41%|████      | 146/356 [00:44<01:02,  3.38ba/s][A[A[A
Running tokenizer on train dataset #1:  41%|████      | 146/356 [00:44<01:02,  3.37ba/s][ARunning tokenizer on train dataset #0:  41%|████      | 146/356 [00:44<01:02,  3.34ba/s]

Running tokenizer on train dataset #2:  41%|████      | 146/356 [00:44<01:02,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  41%|████▏     | 147/356 [00:45<01:01,  3.38ba/s][A[A[A
Running tokenizer on train dataset #1:  41%|████▏     | 147/356 [00:45<01:01,  3.37ba/s][ARunning tokenizer on train dataset #0:  41%|████▏     | 147/356 [00:45<01:02,  3.35ba/s]

Running tokenizer on train dataset #2:  41%|████▏     | 147/356 [00:45<01:02,  3.34ba/s][A[A


Running tokenizer on train dataset #3:  42%|████▏     | 148/356 [00:45<01:01,  3.39ba/s][A[A[A
Running tokenizer on train dataset #1:  42%|████▏     | 148/356 [00:45<01:01,  3.37ba/s][ARunning tokenizer on train dataset #0:  42%|████▏     | 148/356 [00:45<01:01,  3.37ba/s]

Running tokenizer on train dataset #2:  42%|████▏     | 148/356 [00:45<01:01,  3.37ba/s][A[A


Running tokenizer on train dataset #3:  42%|████▏     | 149/356 [00:45<01:04,  3.19ba/s][A[A[A
Running tokenizer on train dataset #1:  42%|████▏     | 149/356 [00:45<01:05,  3.17ba/s][ARunning tokenizer on train dataset #0:  42%|████▏     | 149/356 [00:45<01:05,  3.18ba/s]

Running tokenizer on train dataset #2:  42%|████▏     | 149/356 [00:45<01:05,  3.16ba/s][A[A


Running tokenizer on train dataset #3:  42%|████▏     | 150/356 [00:45<01:03,  3.25ba/s][A[A[A
Running tokenizer on train dataset #1:  42%|████▏     | 150/356 [00:46<01:03,  3.23ba/s][ARunning tokenizer on train dataset #0:  42%|████▏     | 150/356 [00:46<01:03,  3.26ba/s]

Running tokenizer on train dataset #2:  42%|████▏     | 150/356 [00:46<01:03,  3.23ba/s][A[A


Running tokenizer on train dataset #3:  42%|████▏     | 151/356 [00:46<01:02,  3.29ba/s][A[A[A
Running tokenizer on train dataset #1:  42%|████▏     | 151/356 [00:46<01:02,  3.28ba/s][ARunning tokenizer on train dataset #0:  42%|████▏     | 151/356 [00:46<01:02,  3.29ba/s]

Running tokenizer on train dataset #2:  42%|████▏     | 151/356 [00:46<01:03,  3.25ba/s][A[A


Running tokenizer on train dataset #3:  43%|████▎     | 152/356 [00:46<01:01,  3.31ba/s][A[A[A
Running tokenizer on train dataset #1:  43%|████▎     | 152/356 [00:46<01:01,  3.29ba/s][ARunning tokenizer on train dataset #0:  43%|████▎     | 152/356 [00:46<01:01,  3.32ba/s]

Running tokenizer on train dataset #2:  43%|████▎     | 152/356 [00:46<01:01,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  43%|████▎     | 153/356 [00:46<01:00,  3.35ba/s][A[A[ARunning tokenizer on train dataset #0:  43%|████▎     | 153/356 [00:46<01:00,  3.36ba/s]
Running tokenizer on train dataset #1:  43%|████▎     | 153/356 [00:46<01:01,  3.30ba/s][A

Running tokenizer on train dataset #2:  43%|████▎     | 153/356 [00:46<01:01,  3.31ba/s][A[A


Running tokenizer on train dataset #3:  43%|████▎     | 154/356 [00:47<01:00,  3.36ba/s][A[A[ARunning tokenizer on train dataset #0:  43%|████▎     | 154/356 [00:47<00:59,  3.38ba/s]
Running tokenizer on train dataset #1:  43%|████▎     | 154/356 [00:47<01:00,  3.33ba/s][A

Running tokenizer on train dataset #2:  43%|████▎     | 154/356 [00:47<01:00,  3.33ba/s][A[A


Running tokenizer on train dataset #3:  44%|████▎     | 155/356 [00:47<00:59,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  44%|████▎     | 155/356 [00:47<00:59,  3.37ba/s]
Running tokenizer on train dataset #1:  44%|████▎     | 155/356 [00:47<01:00,  3.34ba/s][A

Running tokenizer on train dataset #2:  44%|████▎     | 155/356 [00:47<01:00,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  44%|████▍     | 156/356 [00:47<00:58,  3.40ba/s][A[A[ARunning tokenizer on train dataset #0:  44%|████▍     | 156/356 [00:47<00:59,  3.37ba/s]
Running tokenizer on train dataset #1:  44%|████▍     | 156/356 [00:47<00:59,  3.35ba/s][A

Running tokenizer on train dataset #2:  44%|████▍     | 156/356 [00:47<00:59,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  44%|████▍     | 157/356 [00:48<00:59,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  44%|████▍     | 157/356 [00:48<00:58,  3.38ba/s]
Running tokenizer on train dataset #1:  44%|████▍     | 157/356 [00:48<00:59,  3.36ba/s][A

Running tokenizer on train dataset #2:  44%|████▍     | 157/356 [00:48<00:59,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  44%|████▍     | 158/356 [00:48<00:58,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  44%|████▍     | 158/356 [00:48<00:58,  3.37ba/s]
Running tokenizer on train dataset #1:  44%|████▍     | 158/356 [00:48<00:59,  3.34ba/s][A

Running tokenizer on train dataset #2:  44%|████▍     | 158/356 [00:48<00:59,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  45%|████▍     | 159/356 [00:48<00:58,  3.37ba/s][A[A[ARunning tokenizer on train dataset #0:  45%|████▍     | 159/356 [00:48<00:58,  3.37ba/s]
Running tokenizer on train dataset #1:  45%|████▍     | 159/356 [00:48<00:58,  3.36ba/s][A

Running tokenizer on train dataset #2:  45%|████▍     | 159/356 [00:48<00:58,  3.37ba/s][A[A


Running tokenizer on train dataset #3:  45%|████▍     | 160/356 [00:49<01:01,  3.19ba/s][A[A[ARunning tokenizer on train dataset #0:  45%|████▍     | 160/356 [00:49<01:01,  3.17ba/s]
Running tokenizer on train dataset #1:  45%|████▍     | 160/356 [00:49<01:01,  3.16ba/s][A

Running tokenizer on train dataset #2:  45%|████▍     | 160/356 [00:49<01:02,  3.16ba/s][A[A


Running tokenizer on train dataset #3:  45%|████▌     | 161/356 [00:49<01:00,  3.24ba/s][A[A[A
Running tokenizer on train dataset #1:  45%|████▌     | 161/356 [00:49<01:00,  3.23ba/s][ARunning tokenizer on train dataset #0:  45%|████▌     | 161/356 [00:49<01:00,  3.21ba/s]

Running tokenizer on train dataset #2:  45%|████▌     | 161/356 [00:49<01:00,  3.22ba/s][A[A


Running tokenizer on train dataset #3:  46%|████▌     | 162/356 [00:49<00:59,  3.28ba/s][A[A[A
Running tokenizer on train dataset #1:  46%|████▌     | 162/356 [00:49<00:59,  3.26ba/s][ARunning tokenizer on train dataset #0:  46%|████▌     | 162/356 [00:49<00:59,  3.24ba/s]

Running tokenizer on train dataset #2:  46%|████▌     | 162/356 [00:49<00:59,  3.25ba/s][A[A


Running tokenizer on train dataset #3:  46%|████▌     | 163/356 [00:49<00:58,  3.31ba/s][A[A[A
Running tokenizer on train dataset #1:  46%|████▌     | 163/356 [00:49<00:58,  3.28ba/s][ARunning tokenizer on train dataset #0:  46%|████▌     | 163/356 [00:49<00:59,  3.27ba/s]

Running tokenizer on train dataset #2:  46%|████▌     | 163/356 [00:49<00:58,  3.29ba/s][A[A


Running tokenizer on train dataset #3:  46%|████▌     | 164/356 [00:50<00:57,  3.34ba/s][A[A[ARunning tokenizer on train dataset #0:  46%|████▌     | 164/356 [00:50<00:57,  3.31ba/s]
Running tokenizer on train dataset #1:  46%|████▌     | 164/356 [00:50<00:57,  3.31ba/s][A

Running tokenizer on train dataset #2:  46%|████▌     | 164/356 [00:50<00:58,  3.30ba/s][A[A


Running tokenizer on train dataset #3:  46%|████▋     | 165/356 [00:50<00:57,  3.35ba/s][A[A[A
Running tokenizer on train dataset #1:  46%|████▋     | 165/356 [00:50<00:57,  3.33ba/s][ARunning tokenizer on train dataset #0:  46%|████▋     | 165/356 [00:50<00:57,  3.32ba/s]

Running tokenizer on train dataset #2:  46%|████▋     | 165/356 [00:50<00:57,  3.34ba/s][A[A


Running tokenizer on train dataset #3:  47%|████▋     | 166/356 [00:50<00:56,  3.36ba/s][A[A[A
Running tokenizer on train dataset #1:  47%|████▋     | 166/356 [00:50<00:56,  3.35ba/s][ARunning tokenizer on train dataset #0:  47%|████▋     | 166/356 [00:50<00:57,  3.32ba/s]

Running tokenizer on train dataset #2:  47%|████▋     | 166/356 [00:50<00:56,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  47%|████▋     | 167/356 [00:51<00:56,  3.37ba/s][A[A[A
Running tokenizer on train dataset #1:  47%|████▋     | 167/356 [00:51<00:56,  3.37ba/s][ARunning tokenizer on train dataset #0:  47%|████▋     | 167/356 [00:51<00:56,  3.33ba/s]

Running tokenizer on train dataset #2:  47%|████▋     | 167/356 [00:51<00:56,  3.35ba/s][A[A


Running tokenizer on train dataset #3:  47%|████▋     | 168/356 [00:51<00:55,  3.38ba/s][A[A[A
Running tokenizer on train dataset #1:  47%|████▋     | 168/356 [00:51<00:55,  3.37ba/s][ARunning tokenizer on train dataset #0:  47%|████▋     | 168/356 [00:51<00:56,  3.34ba/s]

Running tokenizer on train dataset #2:  47%|████▋     | 168/356 [00:51<00:56,  3.36ba/s][A[A


Running tokenizer on train dataset #3:  47%|████▋     | 169/356 [00:51<00:55,  3.36ba/s][A[A[A