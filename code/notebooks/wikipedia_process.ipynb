{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600:589:Ashmore And Cartier Islands\n",
      "600:590:Austin (disambiguation)\n",
      "600:593:Animation\n",
      "600:594:Apollo\n",
      "600:595:Andre Agassi\n",
      "676575:596:Artificial languages\n",
      "676575:597:Austroasiatic languages\n",
      "676575:598:Afro-asiatic languages\n",
      "676575:599:Afroasiatic languages\n",
      "676575:600:Andorra\n"
     ]
    }
   ],
   "source": [
    "!head -105 '/afs/crc.nd.edu/group/dmsquare/vol1/data/WIKIPEDIA/wikipedia_dump/enwiki-20220101-pages-articles-multistream-index.txt' | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_filename = '/afs/crc.nd.edu/group/dmsquare/vol1/data/WIKIPEDIA/wikipedia_dump/enwiki-20220101-pages-articles-multistream-index.txt'\n",
    "wiki_filename = '/afs/crc.nd.edu/group/dmsquare/vol1/data/WIKIPEDIA/wikipedia_dump/enwiki-20220101-pages-articles-multistream.xml.bz2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def search_index(search_term, index_filename):\n",
    "    byte_flag = False\n",
    "    data_length = start_byte = 0\n",
    "    index_file = open(index_filename, 'r')\n",
    "    csv_reader = csv.reader(index_file, delimiter=':')\n",
    "    for line in csv_reader:\n",
    "        if not byte_flag and search_term == line[2]:\n",
    "            start_byte = int(line[0])\n",
    "            byte_flag = True\n",
    "        elif byte_flag and int(line[0]) != start_byte:\n",
    "            data_length = int(line[0]) - start_byte\n",
    "            break\n",
    "    index_file.close()\n",
    "    return start_byte, data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = 'Hajee Mohammad Danesh Science & Technology University'\n",
    "index_filename = '/afs/crc.nd.edu/group/dmsquare/vol1/data/WIKIPEDIA/wikipedia_dump/enwiki-20220101-pages-articles-multistream-index.txt'\n",
    "search_index(search_term, index_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import shutil\n",
    "\n",
    "def decompress_chunk(wiki_filename, start_byte, data_length):\n",
    "    temp_filename = 'chunk.bz2'\n",
    "    decomp_filename = 'chunk.xml'\n",
    "\n",
    "    with open(wiki_filename, 'rb') as wiki_file:\n",
    "        wiki_file.seek(start_byte)\n",
    "        data = wiki_file.read(data_length)\n",
    "\n",
    "    with open(temp_filename, 'wb') as temp_file:\n",
    "        temp_file.write(data)\n",
    "\n",
    "    with bz2.BZ2File(temp_filename) as fr, open(decomp_filename,\"wb\") as fw:\n",
    "        shutil.copyfileobj(fr,fw)\n",
    "\n",
    "    return decomp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chunk.xml'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = 'Donald Trump'\n",
    "index_filename = '/afs/crc.nd.edu/group/dmsquare/vol1/data/WIKIPEDIA/wikipedia_dump/enwiki-20220101-pages-articles-multistream-index.txt'\n",
    "wiki_filename = '/afs/crc.nd.edu/group/dmsquare/vol1/data/WIKIPEDIA/wikipedia_dump/enwiki-20220101-pages-articles-multistream.xml.bz2'\n",
    "start_byte, data_length = search_index(search_term, index_filename)\n",
    "decompress_chunk(wiki_filename, start_byte, data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Invalid data stream",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2119225/1663443506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_byte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreadback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpage_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: Invalid data stream"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DUMP_FILE = wiki_filename\n",
    "search_term = \"Hawaii Technology Institute\"\n",
    "start_byte, data_length = search_index(search_term, index_filename)\n",
    "\n",
    "decomp = bz2.BZ2Decompressor()\n",
    "with open(DUMP_FILE, 'rb') as f:\n",
    "    f.seek(start_byte)\n",
    "    readback = f.read(data_length)\n",
    "    page_xml = decomp.decompress(readback).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z5h7ibbjsez2du81</sha1>\\n    </revision>\\n  </page>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_xml[:50]\n",
    "## \"  <page>\\n    <title>Stokell's smelt</title>\\n    <n\"\n",
    "page_xml[-50:]\n",
    "## '4bs3rqxoeidnvw67</sha1>\\n    </revision>\\n  </page>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(page_xml, \"lxml\")\n",
    "pages = soup.find_all(\"page\")\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = [p.find(\"title\").text for p in pages]\n",
    "page_index = page_titles.index(\"Donald Trump\")\n",
    "\n",
    "\n",
    "microbial_fuel_cell_text = pages[page_index].find(\"text\").text\n",
    "microbial_fuel_cell_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiextractor.extract import Extractor\n",
    "extractor = Extractor(\"\", \"\", \"\", \"\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extractor.clean_text(microbial_fuel_cell_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "038ac43ff01c31cd641835814f979bd9daf9dcda5104326cfb2cc592d72d42d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
